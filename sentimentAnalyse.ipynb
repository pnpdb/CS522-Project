{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()  # Models - punkt, Punkt tokenizer Models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plot\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, TrainingArguments, Trainer, TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "from Common.DataCenter import data_center\n",
    "from Common.preprocessor import normalize_preprocessing as normalize\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "class_names = ['Anti', 'Neutral', 'Pro', 'News']\n",
    "class_types = [0, 1, 2, 3]\n",
    "show_confusion_matrix = False\n",
    "outpath = './results/report/'\n",
    "show_plot = False\n",
    "\n",
    "\n",
    "def get_data(file_name, train_size, validation_size, test_size, noisy_size, name='', train_distribution=None, noisy_distribution=None):\n",
    "    ''' Load and preprocess data\n",
    "    Load file from file and remove empty message, drop NaN data, drop duplicate data and shuffle data finally.\n",
    "\n",
    "    Args:\n",
    "        file_name\n",
    "        validation_size\n",
    "        test_size\n",
    "        noisy_size\n",
    "\n",
    "    Returns:\n",
    "        Return different data sets (train, validate, test, mixed_noisy, raw_noisy) which has tow columns(message,sentiment)\n",
    "    '''\n",
    "    dc = data_center(file_name, test_size, noisy_size, validation_size)\n",
    "    dc.add_noisy(noisy_source=\"irrelevant\", distribution=noisy_distribution, size=noisy_size)\n",
    "\n",
    "    noisy_distr = None  # [0.25, 0.25, 0.25, 0.25]\n",
    "    if(noisy_size == 0):\n",
    "        X_train, y_train = dc.get_train(train_size, train_distribution)\n",
    "    else:\n",
    "        X_train, y_train = dc.get_train_with_noisy(train_size, noisy_size, noisy_distr)\n",
    "\n",
    "    X_validate, y_validate = dc.get_validation()\n",
    "    X_test, y_test = dc.get_test()\n",
    "    X_noisy, y_noisy = dc.get_noisy()\n",
    "\n",
    "    dists = data_center.print_distribution('', y_train, False)\n",
    "    dist = {class_names[i]: \"%.1f%%\" % dists[i] for i in range(len(dists))}\n",
    "    print('%s, TrainSet:%s, ValidateSet:%s, TestSet:%s, NoisySet:%s, SentimentDistribution(Training): %s' % (name, len(X_train), len(X_validate), len(X_test), len(X_noisy), dist))\n",
    "\n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test, X_noisy, y_noisy\n",
    "\n",
    "\n",
    "def tokenizer(X_train,  y_train, X_test, y_test, params={}):\n",
    "    ''' Tokenizie the words of the dataset(lecture 4-NLP/5-LSA/5-NLP)\n",
    "    Format/language stripping\n",
    "    Tokenization(language: Chinese and Japanese, Accents, language-specific, Ambiguous)\n",
    "    Normalization(right-to-left, Date, Alphabet)\n",
    "    Punctuation\n",
    "    Numbers\n",
    "    Case folding\n",
    "    Thesauri and soundex\n",
    "    Stop Words Removal\n",
    "    *Lemmatization\n",
    "    Stemming(Porter’s algorithm, Word sense Disambiguation, Machine Translation, Accent Restoration in Spanish & French, Capitalization Restoration, Text-to-Speech Synthesis, Spelling Correction) \n",
    "    Dimensionality Reduction\n",
    "    Latent Semantic Analysis\n",
    "    Docment:\n",
    "    - Sequence Labeling as Classification/Forward Classification/Backward Classification\n",
    "    - Part of Speech Tagging(Maximum Entropy Markov Model)\n",
    "\n",
    "    Word\n",
    "    - Named Entity Recognition\n",
    "    - Information Extraction/The Semantic Web\n",
    "\n",
    "    Text Annotation\n",
    "    - Labeled Dependency Parsing/Dependency Trees\n",
    "\n",
    "    pLSA/LDA\n",
    "    *Word2Vec/Word2Vec Neural Networks\n",
    "    Neural Networks\n",
    "    BERT Deep Learning Network(BERT Word Embeddings/Text Classification/Transformer)\n",
    "    '''\n",
    "    # Normalization\n",
    "    x_train_normalized = X_train\n",
    "    x_test_normalized = X_test\n",
    "    if(params[\"tokenizer\"] == True):\n",
    "        x_train_normalized = normalize(X_train)\n",
    "        x_test_normalized = normalize(X_test)\n",
    "\n",
    "    # One-hot-encoding\n",
    "    y_train_encoded = y_train\n",
    "    y_test_encoded = y_test\n",
    "    if(params[\"one_hot_encoding\"] == True):\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        y_train_encoded = mlb.fit_transform(map(str, y_train))\n",
    "        y_test_encoded = mlb.transform(map(str, y_test))\n",
    "\n",
    "    return x_train_normalized, y_train_encoded, x_test_normalized, y_test_encoded\n",
    "\n",
    "\n",
    "def svm_fit(x_train, y_train, x_test, y_test, kernel='linear', dataset_type='train', params={}):\n",
    "    '''Fit the data by using SVM with linear kernel.'''\n",
    "    if(len(x_train) == 0):\n",
    "        return None\n",
    "\n",
    "    t1 = time.time()\n",
    "    # Preprocess\n",
    "    x_train, y_train, x_test, y_test = tokenizer(x_train, y_train, x_test, y_test, params)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=5, max_df=0.8, sublinear_tf=True, use_idf=True)\n",
    "    x_train_vectors = vectorizer.fit_transform(x_train)\n",
    "    x_test_vectors = vectorizer.transform(x_test)\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Train the data\n",
    "    if (not hasattr(y_train, \"shape\")):\n",
    "        model = svm.SVC(kernel=kernel)\n",
    "        # model = OneVsRestClassifier(svm.SVC(kernel=kernel), n_jobs=-1)\n",
    "    else:\n",
    "        model = OneVsRestClassifier(LinearSVC(dual=False, class_weight='balanced'), n_jobs=-1)\n",
    "    # model = OneVsRestClassifier(svm.SVC(kernel=kernel), n_jobs=-1)\n",
    "\n",
    "    model.fit(x_train_vectors, y_train)\n",
    "    t3 = time.time()\n",
    "\n",
    "    # Predict the test data\n",
    "    y_predict = model.predict(x_test_vectors)\n",
    "    t4 = time.time()\n",
    "\n",
    "    # Report the accurency\n",
    "    report = plot_report(y_test, y_predict, {\"name\": \"SVM-\"+kernel, \"data_type\": dataset_type, \"eval_loss\": 0, \"preprocess\": round(t2-t1, 3), \"train\": round(t3-t2, 3), \"predict\": round(t4-t3, 3)})\n",
    "    if (not hasattr(report, \"accuracy\")):\n",
    "        score = model.score(x_test_vectors, y_test)\n",
    "        report[\"accuracy\"] = score\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def bert_fit(x_train, y_train, x_test, y_test, dataset_type='train', params={}):\n",
    "    '''Fit the data by using BERT. '''\n",
    "\n",
    "    if(len(x_train) == 0):\n",
    "        return None\n",
    "\n",
    "    # Tokenizer the train data and test data\n",
    "    t1 = time.time()\n",
    "    # Preprocess\n",
    "    x_train, y_train, x_test, y_test = tokenizer(x_train, y_train, x_test, y_test, params)\n",
    "\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "    x_train_tokens = tokenizer(list(x_train), truncation=True, padding=True)\n",
    "    x_test_tokens = tokenizer(list(x_test), truncation=True, padding=True)\n",
    "    t2 = time.time()\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((dict(x_train_tokens), y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((dict(x_test_tokens), y_test))\n",
    "\n",
    "    # Prepare the args\n",
    "    training_args = TFTrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=2,              # total number of training epochs\n",
    "        per_device_train_batch_size=16,  # batch size per device during training\n",
    "        per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.1,                # strength of weight decay\n",
    "        logging_steps=100,\n",
    "        eval_steps=10,\n",
    "    )\n",
    "\n",
    "    report = None\n",
    "    with training_args.strategy.scope():\n",
    "        trainer_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "\n",
    "        model = TFTrainer(\n",
    "            model=trainer_model,                 # the instantiated Transformers model to be trained\n",
    "            args=training_args,                  # training arguments, defined above\n",
    "            train_dataset=train_dataset,         # training dataset\n",
    "            eval_dataset=test_dataset,           # evaluation dataset\n",
    "        )\n",
    "\n",
    "        # Train the data\n",
    "        model.train()\n",
    "        t3 = time.time()\n",
    "\n",
    "        # Predict the test data\n",
    "        predictions, y_predict, metrics = model.predict(test_dataset)\n",
    "        t4 = time.time()\n",
    "\n",
    "        # Report the accurency\n",
    "        report = plot_report(y_test, y_predict, {\"name\": \"BERT\", \"data_type\": dataset_type, \"eval_loss\": metrics[\"eval_loss\"], \"preprocess\": round(t2-t1, 3), \"train\": round(t3-t2, 3), \"predict\": round(t4-t3, 3)})\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def plot_report(y_test, y_predict, args=None):\n",
    "    report = classification_report(y_test, y_predict, output_dict=True, target_names=class_names, digits=3, zero_division=1)\n",
    "    report[\"args\"] = args\n",
    "    if (show_confusion_matrix == True):\n",
    "        cm = confusion_matrix(y_test, y_predict, labels=class_types)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_types)\n",
    "        disp.plot()\n",
    "        plot.show()\n",
    "    return report\n",
    "\n",
    "\n",
    "def get_rand_name():\n",
    "    now = datetime.datetime.now()\n",
    "    dt = datetime.datetime.strftime(now, '%Y%m%d%H%M%S')\n",
    "    rand = random.randint(1, 4)\n",
    "    return \"%s-%s\" % dt, rand\n",
    "\n",
    "\n",
    "def get_report_summary(report):\n",
    "    macro_avg = report[\"macro avg\"]\n",
    "    args = report[\"args\"]\n",
    "    report_simple = {\n",
    "        \"Name\": args[\"name\"],\n",
    "        \"DataType\": args[\"data_type\"],\n",
    "        \"TrainSize\": macro_avg[\"support\"],\n",
    "        \"TestSize\": 0,\n",
    "        \"Accuracy\": report[\"accuracy\"],\n",
    "        # \"Loss\": args[\"eval_loss\"],\n",
    "        \"Macro-F1\": macro_avg[\"f1-score\"],\n",
    "        \"Macro-precision\": macro_avg[\"precision\"],\n",
    "        \"Macro-recall\": macro_avg[\"recall\"],\n",
    "        \"Weighted-F1\":  report[\"weighted avg\"][\"f1-score\"],\n",
    "        # \"Time-Preprocess(s)\": args[\"preprocess\"],\n",
    "        # \"Time-Train(s)\": args[\"train\"],\n",
    "        # \"Time-Predict(s)\": args[\"predict\"]\n",
    "    }\n",
    "    return report_simple\n",
    "\n",
    "\n",
    "def plot_item(report, xvalue_name, yvalue_name, title_value_name='', plot_data_type=[\"train\"]):\n",
    "    # Plot the firgues\n",
    "    names = [item[\"Name\"] for item in report[\"train_report\"]]\n",
    "    names = np.unique(names)\n",
    "\n",
    "    data_types = [item[\"DataType\"] for item in report[\"train_report\"]]\n",
    "    data_types = np.unique(data_types)\n",
    "    data_size = None\n",
    "\n",
    "    for dt in data_types:\n",
    "        if dt not in plot_data_type:\n",
    "            continue\n",
    "        for name in names:\n",
    "            data = [item for item in report[\"train_report\"] if item[\"Name\"] == name and item[\"DataType\"] == dt]\n",
    "            x = [item[xvalue_name] for item in data]\n",
    "            y = [item[yvalue_name] for item in data]\n",
    "            if data_size == None:\n",
    "                data_size = x\n",
    "            plot.plot(x, y, label=name+'-'+dt)\n",
    "\n",
    "    plot.title('%s %s with NoisySet %s' % (report[\"model\"], yvalue_name, report[title_value_name]))\n",
    "    plot.xticks(data_size)\n",
    "    plot.xlabel(xvalue_name)\n",
    "    plot.ylabel(xvalue_name)\n",
    "    plot.legend()\n",
    "    plot.savefig('%s%s-%s-%s.png' % (outpath, report[\"title\"], yvalue_name,get_rand_name()), bbox_inches='tight')\n",
    "    if show_plot:\n",
    "        plot.show()\n",
    "\n",
    "\n",
    "def summary(result):\n",
    "    '''Plot and print the summary of the result.'''\n",
    "    report = {}\n",
    "    train_reports = []\n",
    "    validate_reports = []\n",
    "    train_reports_detail = []\n",
    "    validate_reports_detail = []\n",
    "    report[\"noisy_size\"] = []\n",
    "    for i in range(len(result)):\n",
    "        if i == 0:\n",
    "            report[\"model\"] = result[i][\"model\"]\n",
    "            report[\"index\"] = result[i][\"index\"]\n",
    "            report[\"title\"] = \"%s.%s\" % (report[\"index\"], report[\"model\"])\n",
    "\n",
    "        report[\"noisy_size\"].append(result[i][\"noisy\"])\n",
    "        train_report = get_report_summary(result[i][\"train_report\"])\n",
    "        validate_report = get_report_summary(result[i][\"validate_report\"])\n",
    "        train_report[\"TrainSize\"] = result[i][\"train\"]\n",
    "        validate_report[\"TestSize\"] = result[i][\"test\"]\n",
    "        train_reports.append(train_report)\n",
    "        validate_reports.append(validate_report)\n",
    "\n",
    "        train_reports_detail.append(result[i][\"train_report\"])\n",
    "        validate_reports_detail.append(result[i][\"validate_report\"])\n",
    "\n",
    "    train_reports.sort(key=lambda r: r[\"TrainSize\"]*100000 + r[\"Accuracy\"])\n",
    "    validate_reports.sort(key=lambda r: r[\"TrainSize\"]*100000 + r[\"Accuracy\"])\n",
    "    report['train_report'] = train_reports\n",
    "    report['validate_report'] = validate_reports\n",
    "\n",
    "    train_reports_detail.sort(key=lambda r: -r[\"accuracy\"])\n",
    "    validate_reports_detail.sort(key=lambda r: -r[\"accuracy\"])\n",
    "    report['train_report_detail'] = train_reports_detail\n",
    "    report['validate_report_detail'] = validate_reports_detail\n",
    "\n",
    "    noises = np.unique(report[\"noisy_size\"])\n",
    "    report[\"noisy_size\"] = '-'.join(['%s' % v for v in noises])\n",
    "    report[\"title\"] = \"%s with Noisy %s\" % (report[\"title\"], report[\"noisy_size\"])\n",
    "\n",
    "    train_reports.extend(validate_reports)\n",
    "    simple_result = pd.DataFrame(train_reports)\n",
    "    print(simple_result)\n",
    "\n",
    "    try:\n",
    "        file_name = \"%s%s-%s.txt\" % (outpath, report[\"title\"],get_rand_name())\n",
    "        file_name_json = \"%s%s-%s.json\" % (outpath, report[\"title\"], get_rand_name())\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(\"%s\" % simple_result)\n",
    "        with open(file_name_json, 'w') as f:\n",
    "            json.dump(report, f)\n",
    "    except Exception as e:\n",
    "        print('WriteFileError', e)\n",
    "\n",
    "    # Plot the result\n",
    "    outputs = [\n",
    "        ['TrainSize', 'Accuracy', 'noisy_size'],\n",
    "        ['TrainSize', 'Macro-F1', 'noisy_size'],\n",
    "        ['TrainSize', 'Macro-precision', 'noisy_size'],\n",
    "        ['TrainSize', 'Loss', 'noisy_size']\n",
    "    ]\n",
    "    for s in outputs:\n",
    "        plot_item(report, s[0], s[1], s[2])\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def predict(filename, configs):\n",
    "    '''Predict the data from data file with the parameters from the config of the group.'''\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(configs)):\n",
    "        config = configs[i]\n",
    "        if (config['enabled'] != 1):\n",
    "            continue\n",
    "\n",
    "        model, train_sizes, validate_size, test_size, noisy_sizes, params = config[\"model\"], config[\"train\"], config['validate'], config['test'], config['noisy'], config['params']\n",
    "        for i in range(len(train_sizes)):\n",
    "            name = '%s. %s-%s ' % (i+1, model, params[\"kernel\"])\n",
    "            X_train, y_train, X_validate, y_validate, X_test, y_test, X_noisy, y_noisy = get_data(filename, train_sizes[i], validate_size, test_size, noisy_sizes[i], name)\n",
    "\n",
    "            if(model == \"SVM\"):\n",
    "                # SVM kernels\n",
    "                for j in range(len(params[\"kernel\"])):\n",
    "                    name = '%s. %s-%s ' % (i+1, model, params[\"kernel\"][j])\n",
    "                    report = svm_fit(X_train, y_train, X_test, y_test, params[\"kernel\"][j], \"train\", params)\n",
    "                    report_validate = svm_fit(X_validate, y_validate, X_test, y_test, params[\"kernel\"][j], \"validate\", params)\n",
    "                    result.append({\"index\": i+1, \"model\": model, \"train\": train_sizes[i], \"validate\": validate_size, \"test\": test_size, \"noisy\": noisy_sizes[i], \"train_report\": report, \"validate_report\": report_validate})\n",
    "\n",
    "            else:\n",
    "                name = '%s. %s-%s ' % (i+1, model, \"\")\n",
    "                report = bert_fit(X_train, y_train, X_test, y_test, \"train\", params)\n",
    "                report_validate = bert_fit(X_validate, y_validate, X_test, y_test, \"validate\", params)\n",
    "                result.append({\"index\": i+1, \"model\": model, \"train\": train_sizes[i], \"validate\": validate_size, \"test\": test_size, \"noisy\": noisy_sizes[i], \"train_report\": report, \"validate_report\": report_validate})\n",
    "        # Show the result in figures and tables\n",
    "        summary(result)\n",
    "        result = []  # Empty the result after print the summary\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    file_name = './twitter_sentiment_data.csv'\n",
    "\n",
    "    groups = 7\n",
    "    configs = [\n",
    "        {  # svm -debug\n",
    "            \"enabled\": 0,\n",
    "            \"model\": \"SVM\",\n",
    "            \"train\": [100, 200, 300],\n",
    "            \"validate\": 100,\n",
    "            \"test\": 110,\n",
    "            \"noisy\":[80, 90, 100],\n",
    "            \"params\":{\"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'], \"tokenizer\":True, \"one_hot_encoding\":True}  # Not support 'precomputed'\n",
    "        },\n",
    "        {  # bert -debug\n",
    "            \"enabled\": 0,\n",
    "            \"model\": \"Bert\",\n",
    "            \"train\": [10, 20, 30],\n",
    "            \"validate\": 10,\n",
    "            \"test\": 10,\n",
    "            \"noisy\":[0, 0, 0],\n",
    "            \"params\":{\"kernel\": ''}\n",
    "        },\n",
    "\n",
    "\n",
    "        {  # svm with no tokenizer , one_hot_encoding，无噪音情况下tokenizer和one_hot_encoding会降低精度\n",
    "            \"enabled\": 1,\n",
    "            \"model\": \"SVM\",\n",
    "            \"train\": [2000, 4000, 5000, 8000, 10000, 15000, 20000],\n",
    "            \"validate\": 1000,\n",
    "            \"test\": 4000,\n",
    "            \"noisy\":[0, 0, 0, 0, 0, 0, 0],\n",
    "            \"params\":{\"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'], \"tokenizer\":False, \"one_hot_encoding\":False}\n",
    "        },\n",
    "        {  # svm with tokenizer, one_hot_encoding\n",
    "            \"enabled\": 1,\n",
    "            \"model\": \"SVM\",\n",
    "            \"train\": [2000, 4000, 5000, 8000, 10000, 15000, 20000],\n",
    "            \"validate\": 1000,\n",
    "            \"test\": 4000,\n",
    "            \"noisy\":[0, 0, 0, 0, 0, 0, 0],\n",
    "            \"params\":{\"kernel\": ['linear'], \"tokenizer\":True, \"one_hot_encoding\":True}\n",
    "        },\n",
    "        {  # svm - noisy with no tokenizer , one_hot_encoding\n",
    "            \"enabled\": 1,\n",
    "            \"model\": \"SVM\",\n",
    "            \"train\": [4000,  8000, 15000],\n",
    "            \"validate\": 1000,\n",
    "            \"test\": 4000,\n",
    "            \"noisy\":[1000, 2000, 5000],\n",
    "            \"params\":{\"kernel\": ['linear'], \"tokenizer\":False, \"one_hot_encoding\":False}\n",
    "        },\n",
    "        {  # svm - noisy with tokenizer , one_hot_encoding\n",
    "            \"enabled\": 1,\n",
    "            \"model\": \"SVM\",\n",
    "            \"train\": [4000,  8000, 15000],\n",
    "            \"validate\": 1000,\n",
    "            \"test\": 4000,\n",
    "            \"noisy\":[1000, 2000, 5000],\n",
    "            \"params\":{\"kernel\": ['linear'], \"tokenizer\":True, \"one_hot_encoding\":True}\n",
    "        },\n",
    "        {  # bert\n",
    "            \"enabled\": 0,\n",
    "            \"model\": \"BERT\",\n",
    "            \"train\": [2000, 4000, 5000, 8000, 10000, 15000, 20000],\n",
    "            \"validate\": 1000,\n",
    "            \"test\": 4000,\n",
    "            \"noisy\":[0, 0, 0, 0, 0, 0, 0],\n",
    "            \"params\":{\"kernel\": ''}\n",
    "        },\n",
    "        {  # bert - noisy\n",
    "            \"enabled\": 0,\n",
    "            \"model\": \"BERT\",\n",
    "            \"train\": [2000, 4000, 5000, 8000, 10000, 15000, 20000],\n",
    "            \"validate\": 1000,\n",
    "            \"test\": 4000,\n",
    "            \"noisy\":[0, 1000, 0, 2000, 0, 5000, 0],\n",
    "            \"params\":{\"kernel\": ''}\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    predict(file_name, configs)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
