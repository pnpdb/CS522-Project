-----------------------------------------------no Normalization without clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.4548 , weighted_f1: 0.5650, macro_precision: 0.6669, macro_recall: 0.4090
Training set size: 2500 samples (12.5%): 
 macro_f1: 0.4779 , weighted_f1: 0.5797, macro_precision: 0.6754, macro_recall: 0.4288
Training set size: 4000 samples (20.0%): 
 macro_f1: 0.5201 , weighted_f1: 0.6103, macro_precision: 0.7001, macro_recall: 0.4690
Training set size: 5000 samples (25.0%): 
 macro_f1: 0.5402 , weighted_f1: 0.6248, macro_precision: 0.7028, macro_recall: 0.4890
Training set size: 7500 samples (37.4%): 
 macro_f1: 0.5707 , weighted_f1: 0.6467, macro_precision: 0.7131, macro_recall: 0.5161
Training set size: 10000 samples (49.9%): 
 macro_f1: 0.5893 , weighted_f1: 0.6561, macro_precision: 0.7196, macro_recall: 0.5355
-----------------------------------------------
Noisy training set size: 2500 samples (2000 original, 500 noisy)
 macro_f1: 0.4179 , weighted_f1: 0.5145, macro_precision: 0.6103, macro_recall: 0.3463
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.4629 , weighted_f1: 0.5503, macro_precision: 0.6246, macro_recall: 0.3929
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.4716 , weighted_f1: 0.5492, macro_precision: 0.6227, macro_recall: 0.3941


----------------------------------------------- no Normalization with clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.4645 , weighted_f1: 0.5678, macro_precision: 0.6614, macro_recall: 0.4193
Training set size: 2500 samples (12.6%): 
 macro_f1: 0.4817 , weighted_f1: 0.5809, macro_precision: 0.6646, macro_recall: 0.4360
Training set size: 4000 samples (20.1%): 
 macro_f1: 0.5236 , weighted_f1: 0.6112, macro_precision: 0.6783, macro_recall: 0.4759
Training set size: 5000 samples (25.1%): 
 macro_f1: 0.5384 , weighted_f1: 0.6197, macro_precision: 0.6896, macro_recall: 0.4866
Training set size: 7500 samples (37.7%): 
 macro_f1: 0.5668 , weighted_f1: 0.6389, macro_precision: 0.7054, macro_recall: 0.5142
Training set size: 10000 samples (50.2%): 
 macro_f1: 0.5800 , weighted_f1: 0.6488, macro_precision: 0.7028, macro_recall: 0.5282
-----------------------------------------------
Noisy training set size: 2499 samples (2000 original, 500 noisy)
 macro_f1: 0.4097 , weighted_f1: 0.5043, macro_precision: 0.6129, macro_recall: 0.3383
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.4630 , weighted_f1: 0.5485, macro_precision: 0.6261, macro_recall: 0.3928
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.4718 , weighted_f1: 0.5451, macro_precision: 0.6243, macro_recall: 0.3928


 ------------------------------ lowercase, tokenize, stemming,punctuation punctation without clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.4659 , weighted_f1: 0.5706, macro_precision: 0.6628, macro_recall: 0.4179
Training set size: 2500 samples (12.5%): 
 macro_f1: 0.4862 , weighted_f1: 0.5819, macro_precision: 0.6706, macro_recall: 0.4357
Training set size: 4000 samples (20.0%): 
 macro_f1: 0.5317 , weighted_f1: 0.6146, macro_precision: 0.6952, macro_recall: 0.4799
Training set size: 5000 samples (25.0%): 
 macro_f1: 0.5440 , weighted_f1: 0.6243, macro_precision: 0.6923, macro_recall: 0.4950
Training set size: 7500 samples (37.4%): 
 macro_f1: 0.5756 , weighted_f1: 0.6459, macro_precision: 0.7118, macro_recall: 0.5211
Training set size: 10000 samples (49.9%): 
 macro_f1: 0.5869 , weighted_f1: 0.6534, macro_precision: 0.7051, macro_recall: 0.5368
-----------------------------------------------
Noisy training set size: 2500 samples (2000 original, 500 noisy)
 macro_f1: 0.4227 , weighted_f1: 0.5087, macro_precision: 0.6010, macro_recall: 0.3496
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.4681 , weighted_f1: 0.5519, macro_precision: 0.6288, macro_recall: 0.3984
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.4628 , weighted_f1: 0.5428, macro_precision: 0.6174, macro_recall: 0.3880


------------------------------ lowercase, tokenize, stemming,punctuation punctation with clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.4725 , weighted_f1: 0.5717, macro_precision: 0.6499, macro_recall: 0.4261
Training set size: 2500 samples (12.6%): 
 macro_f1: 0.4917 , weighted_f1: 0.5840, macro_precision: 0.6557, macro_recall: 0.4438
Training set size: 4000 samples (20.1%): 
 macro_f1: 0.5298 , weighted_f1: 0.6135, macro_precision: 0.6753, macro_recall: 0.4813
Training set size: 5000 samples (25.1%): 
 macro_f1: 0.5446 , weighted_f1: 0.6239, macro_precision: 0.6826, macro_recall: 0.4941
Training set size: 7500 samples (37.7%): 
 macro_f1: 0.5661 , weighted_f1: 0.6387, macro_precision: 0.6876, macro_recall: 0.5176
Training set size: 10000 samples (50.2%): 
 macro_f1: 0.5856 , weighted_f1: 0.6531, macro_precision: 0.7033, macro_recall: 0.5374
-----------------------------------------------
Noisy training set size: 2499 samples (2000 original, 500 noisy)
 macro_f1: 0.4068 , weighted_f1: 0.5031, macro_precision: 0.5855, macro_recall: 0.3392
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.4603 , weighted_f1: 0.5463, macro_precision: 0.6154, macro_recall: 0.3919
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.4749 , weighted_f1: 0.5491, macro_precision: 0.6266, macro_recall: 0.3955

----------------------------------------------- add remove stopwords and pos-tag without clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3945 , weighted_f1: 0.4981, macro_precision: 0.5678, macro_recall: 0.3560
Training set size: 2500 samples (12.5%): 
 macro_f1: 0.3920 , weighted_f1: 0.4933, macro_precision: 0.5478, macro_recall: 0.3596
Training set size: 4000 samples (20.0%): 
 macro_f1: 0.4140 , weighted_f1: 0.5082, macro_precision: 0.5675, macro_recall: 0.3724
Training set size: 5000 samples (25.0%): 
 macro_f1: 0.4303 , weighted_f1: 0.5190, macro_precision: 0.5595, macro_recall: 0.3902
Training set size: 7500 samples (37.4%): 
 macro_f1: 0.4519 , weighted_f1: 0.5325, macro_precision: 0.5705, macro_recall: 0.4098
Training set size: 10000 samples (49.9%): 
 macro_f1: 0.4639 , weighted_f1: 0.5431, macro_precision: 0.5816, macro_recall: 0.4194
-----------------------------------------------
Noisy training set size: 2500 samples (2000 original, 500 noisy)
 macro_f1: 0.3291 , weighted_f1: 0.4230, macro_precision: 0.4405, macro_recall: 0.2832
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.3698 , weighted_f1: 0.4601, macro_precision: 0.5020, macro_recall: 0.3159
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.3733 , weighted_f1: 0.4520, macro_precision: 0.5037, macro_recall: 0.3105



----------------------------------------------- add remove stopwords and pos-tag with clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.4235 , weighted_f1: 0.5144, macro_precision: 0.5762, macro_recall: 0.3738
Training set size: 2500 samples (12.6%): 
 macro_f1: 0.4336 , weighted_f1: 0.5235, macro_precision: 0.5866, macro_recall: 0.3837
Training set size: 4000 samples (20.1%): 
 macro_f1: 0.4688 , weighted_f1: 0.5489, macro_precision: 0.6064, macro_recall: 0.4168
Training set size: 5000 samples (25.1%): 
 macro_f1: 0.4820 , weighted_f1: 0.5578, macro_precision: 0.6038, macro_recall: 0.4320
Training set size: 7500 samples (37.7%): 
 macro_f1: 0.4992 , weighted_f1: 0.5748, macro_precision: 0.6296, macro_recall: 0.4468
Training set size: 10000 samples (50.2%): 
 macro_f1: 0.5084 , weighted_f1: 0.5818, macro_precision: 0.6195, macro_recall: 0.4610
-----------------------------------------------
Noisy training set size: 2499 samples (2000 original, 500 noisy)
 macro_f1: 0.3719 , weighted_f1: 0.4543, macro_precision: 0.4983, macro_recall: 0.3088
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.4130 , weighted_f1: 0.4911, macro_precision: 0.5470, macro_recall: 0.3474
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.4166 , weighted_f1: 0.4905, macro_precision: 0.5399, macro_recall: 0.3474


----------------------------------------------- only remove stop words without clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.4495 , weighted_f1: 0.5495, macro_precision: 0.6634, macro_recall: 0.3991
Training set size: 2500 samples (12.5%): 
 macro_f1: 0.4680 , weighted_f1: 0.5604, macro_precision: 0.6479, macro_recall: 0.4166
Training set size: 4000 samples (20.0%): 
 macro_f1: 0.5119 , weighted_f1: 0.5939, macro_precision: 0.6753, macro_recall: 0.4609
Training set size: 5000 samples (25.0%): 
 macro_f1: 0.5269 , weighted_f1: 0.6021, macro_precision: 0.6698, macro_recall: 0.4792
Training set size: 7500 samples (37.4%): 
 macro_f1: 0.5566 , weighted_f1: 0.6247, macro_precision: 0.6903, macro_recall: 0.5063
Training set size: 10000 samples (49.9%): 
 macro_f1: 0.5714 , weighted_f1: 0.6348, macro_precision: 0.6890, macro_recall: 0.5232
-----------------------------------------------
Noisy training set size: 2500 samples (2000 original, 500 noisy)
 macro_f1: 0.4089 , weighted_f1: 0.4933, macro_precision: 0.5786, macro_recall: 0.3387
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.4529 , weighted_f1: 0.5322, macro_precision: 0.6048, macro_recall: 0.3870
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.4537 , weighted_f1: 0.5287, macro_precision: 0.5953, macro_recall: 0.3832


----------------------------------------------- only remove stop words with clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.4581 , weighted_f1: 0.5536, macro_precision: 0.6344, macro_recall: 0.4098
Training set size: 2500 samples (12.6%): 
 macro_f1: 0.4709 , weighted_f1: 0.5621, macro_precision: 0.6408, macro_recall: 0.4210
Training set size: 4000 samples (20.1%): 
 macro_f1: 0.5152 , weighted_f1: 0.5964, macro_precision: 0.6654, macro_recall: 0.4653
Training set size: 5000 samples (25.1%): 
 macro_f1: 0.5197 , weighted_f1: 0.6010, macro_precision: 0.6593, macro_recall: 0.4701
Training set size: 7500 samples (37.7%): 
 macro_f1: 0.5512 , weighted_f1: 0.6236, macro_precision: 0.6802, macro_recall: 0.5007
Training set size: 10000 samples (50.2%): 
 macro_f1: 0.5741 , weighted_f1: 0.6399, macro_precision: 0.6932, macro_recall: 0.5240
-----------------------------------------------
Noisy training set size: 2499 samples (2000 original, 500 noisy)
 macro_f1: 0.4015 , weighted_f1: 0.4898, macro_precision: 0.5762, macro_recall: 0.3319
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.4508 , weighted_f1: 0.5319, macro_precision: 0.6041, macro_recall: 0.3811
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.4656 , weighted_f1: 0.5370, macro_precision: 0.6051, macro_recall: 0.3896


----------------------------------------------- climate training word2vec 300d wo clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3981 , weighted_f1: 0.5333, macro_precision: 0.6389, macro_recall: 0.3535
Training set size: 2500 samples (12.5%): 
 macro_f1: 0.3947 , weighted_f1: 0.5312, macro_precision: 0.6000, macro_recall: 0.3526
Training set size: 4000 samples (20.0%): 
 macro_f1: 0.3946 , weighted_f1: 0.5316, macro_precision: 0.5239, macro_recall: 0.3523
Training set size: 5000 samples (25.0%): 
 macro_f1: 0.3974 , weighted_f1: 0.5346, macro_precision: 0.5273, macro_recall: 0.3557
Training set size: 7500 samples (37.4%): 
 macro_f1: 0.3933 , weighted_f1: 0.5335, macro_precision: 0.5311, macro_recall: 0.3509
Training set size: 10000 samples (49.9%): 
 macro_f1: 0.3962 , weighted_f1: 0.5366, macro_precision: 0.5365, macro_recall: 0.3535
-----------------------------------------------
Noisy training set size: 2500 samples (2000 original, 500 noisy)
 macro_f1: 0.3170 , weighted_f1: 0.4446, macro_precision: 0.5577, macro_recall: 0.2414
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.3137 , weighted_f1: 0.4426, macro_precision: 0.5880, macro_recall: 0.2378
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.2664 , weighted_f1: 0.3888, macro_precision: 0.6071, macro_recall: 0.1900


----------------------------------------------- climate training word2vec 300d with clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3855 , weighted_f1: 0.5231, macro_precision: 0.5333, macro_recall: 0.3466
Training set size: 2500 samples (12.6%): 
 macro_f1: 0.3906 , weighted_f1: 0.5296, macro_precision: 0.5355, macro_recall: 0.3516
Training set size: 4000 samples (20.1%): 
 macro_f1: 0.3934 , weighted_f1: 0.5324, macro_precision: 0.5409, macro_recall: 0.3538
Training set size: 5000 samples (25.1%): 
 macro_f1: 0.3926 , weighted_f1: 0.5323, macro_precision: 0.5398, macro_recall: 0.3533
Training set size: 7500 samples (37.7%): 
 macro_f1: 0.4004 , weighted_f1: 0.5398, macro_precision: 0.5457, macro_recall: 0.3593
Training set size: 10000 samples (50.2%): 
 macro_f1: 0.4026 , weighted_f1: 0.5426, macro_precision: 0.5423, macro_recall: 0.3629
-----------------------------------------------
Noisy training set size: 2499 samples (2000 original, 500 noisy)
 macro_f1: 0.3002 , weighted_f1: 0.4284, macro_precision: 0.5837, macro_recall: 0.2287
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.3073 , weighted_f1: 0.4391, macro_precision: 0.5895, macro_recall: 0.2367
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.2726 , weighted_f1: 0.3966, macro_precision: 0.6276, macro_recall: 0.1951



----------------------------------------------- climate training word2vec 100d wo clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3977 , weighted_f1: 0.5310, macro_precision: 0.6358, macro_recall: 0.3519
Training set size: 2500 samples (12.5%): 
 macro_f1: 0.3985 , weighted_f1: 0.5314, macro_precision: 0.6058, macro_recall: 0.3543
Training set size: 4000 samples (20.0%): 
 macro_f1: 0.3958 , weighted_f1: 0.5288, macro_precision: 0.5207, macro_recall: 0.3521
Training set size: 5000 samples (25.0%): 
 macro_f1: 0.3965 , weighted_f1: 0.5320, macro_precision: 0.6500, macro_recall: 0.3534
Training set size: 7500 samples (37.4%): 
 macro_f1: 0.3952 , weighted_f1: 0.5312, macro_precision: 0.6156, macro_recall: 0.3503
Training set size: 10000 samples (49.9%): 
 macro_f1: 0.3955 , weighted_f1: 0.5336, macro_precision: 0.5340, macro_recall: 0.3520
-----------------------------------------------
Noisy training set size: 2500 samples (2000 original, 500 noisy)
 macro_f1: 0.3246 , weighted_f1: 0.4494, macro_precision: 0.6866, macro_recall: 0.2469
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.3156 , weighted_f1: 0.4422, macro_precision: 0.5706, macro_recall: 0.2398
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.2666 , weighted_f1: 0.3875, macro_precision: 0.5961, macro_recall: 0.1897



----------------------------------------------- climate training word2vec 100d with clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3816 , weighted_f1: 0.5181, macro_precision: 0.5785, macro_recall: 0.3444
Training set size: 2500 samples (12.6%): 
 macro_f1: 0.3867 , weighted_f1: 0.5247, macro_precision: 0.5774, macro_recall: 0.3500
Training set size: 4000 samples (20.1%): 
 macro_f1: 0.3892 , weighted_f1: 0.5272, macro_precision: 0.6562, macro_recall: 0.3517
Training set size: 5000 samples (25.1%): 
 macro_f1: 0.3898 , weighted_f1: 0.5282, macro_precision: 0.5978, macro_recall: 0.3512
Training set size: 7500 samples (37.7%): 
 macro_f1: 0.3971 , weighted_f1: 0.5347, macro_precision: 0.7045, macro_recall: 0.3564
Training set size: 10000 samples (50.2%): 
 macro_f1: 0.3998 , weighted_f1: 0.5366, macro_precision: 0.6170, macro_recall: 0.3600
-----------------------------------------------
Noisy training set size: 2499 samples (2000 original, 500 noisy)
 macro_f1: 0.3010 , weighted_f1: 0.4274, macro_precision: 0.5830, macro_recall: 0.2313
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.3055 , weighted_f1: 0.4368, macro_precision: 0.5834, macro_recall: 0.2350
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.2690 , weighted_f1: 0.3910, macro_precision: 0.6043, macro_recall: 0.1925



----------------------------------------------- climate training word2vec 50d wo clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3919 , weighted_f1: 0.5286, macro_precision: 0.5183, macro_recall: 0.3490
Training set size: 2500 samples (12.5%): 
 macro_f1: 0.3922 , weighted_f1: 0.5292, macro_precision: 0.5144, macro_recall: 0.3528
Training set size: 4000 samples (20.0%): 
 macro_f1: 0.3930 , weighted_f1: 0.5286, macro_precision: 0.5204, macro_recall: 0.3518
Training set size: 5000 samples (25.0%): 
 macro_f1: 0.3906 , weighted_f1: 0.5279, macro_precision: 0.5257, macro_recall: 0.3487
Training set size: 7500 samples (37.4%): 
 macro_f1: 0.3887 , weighted_f1: 0.5277, macro_precision: 0.5294, macro_recall: 0.3468
Training set size: 10000 samples (49.9%): 
 macro_f1: 0.3880 , weighted_f1: 0.5275, macro_precision: 0.5334, macro_recall: 0.3469
-----------------------------------------------
Noisy training set size: 2500 samples (2000 original, 500 noisy)
 macro_f1: 0.3110 , weighted_f1: 0.4380, macro_precision: 0.5617, macro_recall: 0.2355
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.2996 , weighted_f1: 0.4320, macro_precision: 0.5693, macro_recall: 0.2291
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.2528 , weighted_f1: 0.3734, macro_precision: 0.6124, macro_recall: 0.1789


----------------------------------------------- climate training word2vec 50d with clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3773 , weighted_f1: 0.5145, macro_precision: 0.5275, macro_recall: 0.3410
Training set size: 2500 samples (12.6%): 
 macro_f1: 0.3806 , weighted_f1: 0.5196, macro_precision: 0.5336, macro_recall: 0.3438
Training set size: 4000 samples (20.1%): 
 macro_f1: 0.3848 , weighted_f1: 0.5232, macro_precision: 0.5356, macro_recall: 0.3460
Training set size: 5000 samples (25.1%): 
 macro_f1: 0.3849 , weighted_f1: 0.5242, macro_precision: 0.5336, macro_recall: 0.3469
Training set size: 7500 samples (37.7%): 
 macro_f1: 0.3873 , weighted_f1: 0.5259, macro_precision: 0.5346, macro_recall: 0.3483
Training set size: 10000 samples (50.2%): 
 macro_f1: 0.3896 , weighted_f1: 0.5276, macro_precision: 0.5310, macro_recall: 0.3505
-----------------------------------------------
Noisy training set size: 2499 samples (2000 original, 500 noisy)
 macro_f1: 0.2876 , weighted_f1: 0.4096, macro_precision: 0.5875, macro_recall: 0.2167
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.2958 , weighted_f1: 0.4260, macro_precision: 0.5910, macro_recall: 0.2257
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.2554 , weighted_f1: 0.3725, macro_precision: 0.6095, macro_recall: 0.1802



----------------------------------------------- GoogleNews corpus 300d wo clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.4213 , weighted_f1: 0.5367, macro_precision: 0.6455, macro_recall: 0.3621
Training set size: 2500 samples (12.5%): 
 macro_f1: 0.4333 , weighted_f1: 0.5456, macro_precision: 0.6652, macro_recall: 0.3700
Training set size: 4000 samples (20.0%): 
 macro_f1: 0.4422 , weighted_f1: 0.5535, macro_precision: 0.6923, macro_recall: 0.3792
Training set size: 5000 samples (25.0%): 
 macro_f1: 0.4468 , weighted_f1: 0.5589, macro_precision: 0.6882, macro_recall: 0.3848
Training set size: 7500 samples (37.4%): 
 macro_f1: 0.4546 , weighted_f1: 0.5619, macro_precision: 0.6922, macro_recall: 0.3900
Training set size: 10000 samples (49.9%): 
 macro_f1: 0.4466 , weighted_f1: 0.5588, macro_precision: 0.6944, macro_recall: 0.3844
-----------------------------------------------
Noisy training set size: 2500 samples (2000 original, 500 noisy)
 macro_f1: 0.3311 , weighted_f1: 0.4453, macro_precision: 0.6314, macro_recall: 0.2505
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.3301 , weighted_f1: 0.4535, macro_precision: 0.6975, macro_recall: 0.2518
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.2878 , weighted_f1: 0.4082, macro_precision: 0.7524, macro_recall: 0.2056



----------------------------------------------- GoogleNews corpus 300d with clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.4157 , weighted_f1: 0.5314, macro_precision: 0.6454, macro_recall: 0.3628
Training set size: 2500 samples (12.6%): 
 macro_f1: 0.4265 , weighted_f1: 0.5395, macro_precision: 0.6485, macro_recall: 0.3720
Training set size: 4000 samples (20.1%): 
 macro_f1: 0.4378 , weighted_f1: 0.5479, macro_precision: 0.6755, macro_recall: 0.3765
Training set size: 5000 samples (25.1%): 
 macro_f1: 0.4428 , weighted_f1: 0.5510, macro_precision: 0.6826, macro_recall: 0.3818
Training set size: 7500 samples (37.7%): 
 macro_f1: 0.4455 , weighted_f1: 0.5565, macro_precision: 0.6929, macro_recall: 0.3855
Training set size: 10000 samples (50.2%): 
 macro_f1: 0.4434 , weighted_f1: 0.5569, macro_precision: 0.6996, macro_recall: 0.3849
-----------------------------------------------
Noisy training set size: 2499 samples (2000 original, 500 noisy)
 macro_f1: 0.3235 , weighted_f1: 0.4360, macro_precision: 0.6282, macro_recall: 0.2472
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.3274 , weighted_f1: 0.4482, macro_precision: 0.6932, macro_recall: 0.2497
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.3062 , weighted_f1: 0.4183, macro_precision: 0.7507, macro_recall: 0.2204



----------------------------------------------- twitter glove 200d wo clean
Training set size: 2000 samples (10.0%):  
 macro_f1: 0.4344 , weighted_f1: 0.5393, macro_precision: 0.5908, macro_recall: 0.3813
Training set size: 2500 samples (12.5%): 
 macro_f1: 0.4321 , weighted_f1: 0.5405, macro_precision: 0.6152, macro_recall: 0.3772
Training set size: 4000 samples (20.0%): 
 macro_f1: 0.4328 , weighted_f1: 0.5457, macro_precision: 0.6766, macro_recall: 0.3760
Training set size: 5000 samples (25.0%): 
 macro_f1: 0.4293 , weighted_f1: 0.5456, macro_precision: 0.6941, macro_recall: 0.3748
Training set size: 7500 samples (37.4%): 
 macro_f1: 0.4205 , weighted_f1: 0.5443, macro_precision: 0.6976, macro_recall: 0.3685
Training set size: 10000 samples (49.9%): 
 macro_f1: 0.4205 , weighted_f1: 0.5455, macro_precision: 0.6971, macro_recall: 0.3693
-----------------------------------------------
Noisy training set size: 2500 samples (2000 original, 500 noisy)
 macro_f1: 0.3390 , weighted_f1: 0.4482, macro_precision: 0.6271, macro_recall: 0.2623
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.3351 , weighted_f1: 0.4553, macro_precision: 0.7329, macro_recall: 0.2582
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.2741 , weighted_f1: 0.3993, macro_precision: 0.8404, macro_recall: 0.1995



----------------------------------------------- twitter glove 200d with clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.4338 , weighted_f1: 0.5371, macro_precision: 0.5830, macro_recall: 0.3816
Training set size: 2500 samples (12.6%): 
 macro_f1: 0.4263 , weighted_f1: 0.5352, macro_precision: 0.5841, macro_recall: 0.3753
Training set size: 4000 samples (20.1%): 
 macro_f1: 0.4318 , weighted_f1: 0.5454, macro_precision: 0.6270, macro_recall: 0.3787
Training set size: 5000 samples (25.1%): 
 macro_f1: 0.4232 , weighted_f1: 0.5396, macro_precision: 0.6468, macro_recall: 0.3709
Training set size: 7500 samples (37.7%): 
 macro_f1: 0.4194 , weighted_f1: 0.5417, macro_precision: 0.6665, macro_recall: 0.3686
Training set size: 10000 samples (50.2%): 
 macro_f1: 0.4151 , weighted_f1: 0.5434, macro_precision: 0.6700, macro_recall: 0.3678
-----------------------------------------------
Noisy training set size: 2499 samples (2000 original, 500 noisy)
 macro_f1: 0.3384 , weighted_f1: 0.4487, macro_precision: 0.6102, macro_recall: 0.2635
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.3279 , weighted_f1: 0.4544, macro_precision: 0.6779, macro_recall: 0.2567
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.2844 , weighted_f1: 0.4040, macro_precision: 0.6957, macro_recall: 0.2058


----------------------------------------------- twitter glove 100d wo clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3810 , weighted_f1: 0.5092, macro_precision: 0.6655, macro_recall: 0.3367
Training set size: 2500 samples (12.5%): 
 macro_f1: 0.3819 , weighted_f1: 0.5127, macro_precision: 0.6458, macro_recall: 0.3388
Training set size: 4000 samples (20.0%): 
 macro_f1: 0.3846 , weighted_f1: 0.5142, macro_precision: 0.6785, macro_recall: 0.3387
Training set size: 5000 samples (25.0%): 
 macro_f1: 0.3841 , weighted_f1: 0.5160, macro_precision: 0.6898, macro_recall: 0.3392
Training set size: 7500 samples (37.4%): 
 macro_f1: 0.3794 , weighted_f1: 0.5146, macro_precision: 0.6647, macro_recall: 0.3360
Training set size: 10000 samples (49.9%): 
 macro_f1: 0.3809 , weighted_f1: 0.5154, macro_precision: 0.6684, macro_recall: 0.3377
-----------------------------------------------
Noisy training set size: 2500 samples (2000 original, 500 noisy)
 macro_f1: 0.2914 , weighted_f1: 0.4129, macro_precision: 0.7315, macro_recall: 0.2214
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.2938 , weighted_f1: 0.4202, macro_precision: 0.7471, macro_recall: 0.2227
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.2350 , weighted_f1: 0.3578, macro_precision: 0.5958, macro_recall: 0.1669


-----------------------------------------------twitter glove 100d with clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3860 , weighted_f1: 0.5070, macro_precision: 0.6265, macro_recall: 0.3381
Training set size: 2500 samples (12.6%): 
 macro_f1: 0.3829 , weighted_f1: 0.5085, macro_precision: 0.6453, macro_recall: 0.3366
Training set size: 4000 samples (20.1%): 
 macro_f1: 0.3847 , weighted_f1: 0.5147, macro_precision: 0.6788, macro_recall: 0.3399
Training set size: 5000 samples (25.1%): 
 macro_f1: 0.3804 , weighted_f1: 0.5118, macro_precision: 0.7032, macro_recall: 0.3365
Training set size: 7500 samples (37.7%): 
 macro_f1: 0.3743 , weighted_f1: 0.5094, macro_precision: 0.6905, macro_recall: 0.3331
Training set size: 10000 samples (50.2%): 
 macro_f1: 0.3761 , weighted_f1: 0.5122, macro_precision: 0.7207, macro_recall: 0.3361
-----------------------------------------------
Noisy training set size: 2499 samples (2000 original, 500 noisy)
 macro_f1: 0.2917 , weighted_f1: 0.4121, macro_precision: 0.6328, macro_recall: 0.2237
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.2853 , weighted_f1: 0.4133, macro_precision: 0.5744, macro_recall: 0.2192
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.2460 , weighted_f1: 0.3605, macro_precision: 0.6040, macro_recall: 0.1746


 ----------------------------------------------- twitter glove 50d wo clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3466 , weighted_f1: 0.4835, macro_precision: 0.6787, macro_recall: 0.3050
Training set size: 2500 samples (12.5%): 
 macro_f1: 0.3500 , weighted_f1: 0.4859, macro_precision: 0.6444, macro_recall: 0.3100
Training set size: 4000 samples (20.0%): 
 macro_f1: 0.3501 , weighted_f1: 0.4864, macro_precision: 0.6323, macro_recall: 0.3099
Training set size: 5000 samples (25.0%): 
 macro_f1: 0.3497 , weighted_f1: 0.4869, macro_precision: 0.6164, macro_recall: 0.3102
Training set size: 7500 samples (37.4%): 
 macro_f1: 0.3508 , weighted_f1: 0.4897, macro_precision: 0.6466, macro_recall: 0.3112
Training set size: 10000 samples (49.9%): 
 macro_f1: 0.3519 , weighted_f1: 0.4900, macro_precision: 0.6515, macro_recall: 0.3134
-----------------------------------------------
Noisy training set size: 2500 samples (2000 original, 500 noisy)
 macro_f1: 0.2434 , weighted_f1: 0.3662, macro_precision: 0.6789, macro_recall: 0.1790
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.2504 , weighted_f1: 0.3778, macro_precision: 0.5602, macro_recall: 0.1868
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.1963 , weighted_f1: 0.3151, macro_precision: 0.6098, macro_recall: 0.1376



----------------------------------------------- twitter glove 50d with clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3457 , weighted_f1: 0.4784, macro_precision: 0.6607, macro_recall: 0.3093
Training set size: 2500 samples (12.6%): 
 macro_f1: 0.3494 , weighted_f1: 0.4842, macro_precision: 0.6464, macro_recall: 0.3119
Training set size: 4000 samples (20.1%): 
 macro_f1: 0.3472 , weighted_f1: 0.4856, macro_precision: 0.5937, macro_recall: 0.3099
Training set size: 5000 samples (25.1%): 
 macro_f1: 0.3479 , weighted_f1: 0.4857, macro_precision: 0.6378, macro_recall: 0.3096
Training set size: 7500 samples (37.7%): 
 macro_f1: 0.3471 , weighted_f1: 0.4855, macro_precision: 0.5333, macro_recall: 0.3092
Training set size: 10000 samples (50.2%): 
 macro_f1: 0.3484 , weighted_f1: 0.4870, macro_precision: 0.5298, macro_recall: 0.3114
-----------------------------------------------
Noisy training set size: 2499 samples (2000 original, 500 noisy)
 macro_f1: 0.2375 , weighted_f1: 0.3589, macro_precision: 0.6433, macro_recall: 0.1771
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.2439 , weighted_f1: 0.3689, macro_precision: 0.5984, macro_recall: 0.1826
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.1979 , weighted_f1: 0.3097, macro_precision: 0.6098, macro_recall: 0.1372



 ----------------------------------------------- twitter glove 25d wo clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3080 , weighted_f1: 0.4436, macro_precision: 0.5065, macro_recall: 0.2729
Training set size: 2500 samples (12.5%):
 macro_f1: 0.3091 , weighted_f1: 0.4452, macro_precision: 0.5027, macro_recall: 0.2760
Training set size: 4000 samples (20.0%): 
 macro_f1: 0.3145 , weighted_f1: 0.4477, macro_precision: 0.5007, macro_recall: 0.2786
Training set size: 5000 samples (25.0%): 
 macro_f1: 0.3166 , weighted_f1: 0.4509, macro_precision: 0.5075, macro_recall: 0.2805
Training set size: 7500 samples (37.4%): 
 macro_f1: 0.3132 , weighted_f1: 0.4478, macro_precision: 0.5097, macro_recall: 0.2771
Training set size: 10000 samples (49.9%): 
 macro_f1: 0.3129 , weighted_f1: 0.4489, macro_precision: 0.5096, macro_recall: 0.2777
-----------------------------------------------
Noisy training set size: 2500 samples (2000 original, 500 noisy)
 macro_f1: 0.1947 , weighted_f1: 0.3033, macro_precision: 0.5606, macro_recall: 0.1379
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.1952 , weighted_f1: 0.3106, macro_precision: 0.5529, macro_recall: 0.1410
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.1441 , weighted_f1: 0.2436, macro_precision: 0.5913, macro_recall: 0.0967
 

----------------------------------------------- twitter glove 25d with clean
Training set size: 2000 samples (10.0%): 
 macro_f1: 0.3112 , weighted_f1: 0.4453, macro_precision: 0.5303, macro_recall: 0.2789
Training set size: 2500 samples (12.6%): 
 macro_f1: 0.3165 , weighted_f1: 0.4514, macro_precision: 0.5293, macro_recall: 0.2841
Training set size: 4000 samples (20.1%): 
 macro_f1: 0.3163 , weighted_f1: 0.4515, macro_precision: 0.5279, macro_recall: 0.2812
Training set size: 5000 samples (25.1%): 
 macro_f1: 0.3133 , weighted_f1: 0.4484, macro_precision: 0.5271, macro_recall: 0.2783
Training set size: 7500 samples (37.7%): 
 macro_f1: 0.3122 , weighted_f1: 0.4474, macro_precision: 0.5278, macro_recall: 0.2765
Training set size: 10000 samples (50.2%): 
 macro_f1: 0.3125 , weighted_f1: 0.4483, macro_precision: 0.5258, macro_recall: 0.2784
-----------------------------------------------
Noisy training set size: 2499 samples (2000 original, 500 noisy)
 macro_f1: 0.2021 , weighted_f1: 0.3170, macro_precision: 0.5830, macro_recall: 0.1477
Noisy training set size: 5000 samples (4000 original, 1000 noisy)
 macro_f1: 0.2095 , weighted_f1: 0.3274, macro_precision: 0.5800, macro_recall: 0.1530
Noisy training set size: 10000 samples (7500 original, 2500 noisy)
 macro_f1: 0.1569 , weighted_f1: 0.2578, macro_precision: 0.5911, macro_recall: 0.1059















































