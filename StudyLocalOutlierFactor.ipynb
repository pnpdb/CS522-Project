{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79799332-2277-4bfe-98b4-58ce2271d693",
   "metadata": {},
   "source": [
    "# Study Local Outlier Factor\n",
    "Local Outlier Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19494804-cc13-4c19-93bf-a67195da55c0",
   "metadata": {},
   "source": [
    "这个算法是基于数据密度进行检测的。首先给定定义\n",
    "\n",
    "<font color=red>k-distance</font>: 距离数据点p最近的几个点中，第k个最近的点跟点p之间的距离。这个是以给定数量的周边数据点作为边界，算出来的\"半径\"。  \n",
    "<font color=red>reachability distance</font>：可达距离。与k-distance相关，给定参数k，p到o的可达距离为o的k-distance与p与o直线距离的最大值，即\n",
    "$$\n",
    "reachdist_k(p,o) = \\max\\{ k-distance(o), d(p,o) \\}\n",
    "$$\n",
    "我们关注的是点p，它到o的可达距离就是直线距离，但是如果这两个点太近了，那么就以目标点o的\"k半径\"为准。直观的理解，就是如果点p要到达点o，就是直线距离，但是距离最低是点o的半径。  \n",
    "<font color=red>local reachability density</font>：局部可达密度，对于与数据点p的距离小于等于k-distance(p)的数据点，就叫做k-nearest-neighbor。记为$N_k(p)$。数据点p的局部可达密度，指的是它与临近的数据点的平均可达距离的倒数，即\n",
    "$$\n",
    "lrd_k(p) = \\frac{1}{\\frac{\\sum_{o \\in N_k(p) }reachdist_k(p,o)}{|N_k(p)|}}\n",
    "$$\n",
    "直观理解就是对于点p而言，其周围的一系列点o，每个点o都是各自的k半径，那么平均可达距离的倒数就是点p的局部可达密度。想象一下，点p如果是比较孤立的，其周围最近的k个点是聚集在一起的，那么reach dist应该就比较大，而$lrd_k(p)$就应该比较小。如果点p是和周围最近的k个点聚集在一起的，那么整体来看reach dist就应该比较小,$lrd_k(p)$就比较大。  \n",
    "<font color=red>local outlier factor</font>：局部异常因子。上面我们已经能算出来每个点的局部可达密度了，不过我们这里专注的不是绝对密度，而是相对密度。这样做的好处就是可以在数据分布不均匀的情况下，也能正常处理数据，不会把一大片数据都看成outlier。\n",
    "$$\n",
    "LOF_k(p) = \\frac{\\sum_{o \\in N_k(p)} \\frac{lrd(o)}{lrd(p)}}{|N_k(p)|} = \\frac{\\sum_{o \\in N_k(p)} lrd(o)}{|N_k(p)|} \\frac{1}{lrd(p)}\n",
    "$$\n",
    "我们可以理解为，数据点p周围的点相对于p的平均密度。这件事的核心在于当前数据点p的\"周围\"与其他数据点o的\"周围\"的范围并不一样，o在p的周围，p不一定在o的周围，只是以一个数量k作为标准。所以，这里就体现出了点p周围的点相对于点p的相对密度的概念。相对密度越大，体现出来的是周围的点o的密度相比于点p是不是更加密集，如果相比之下非常稠密，则意味着当前点p是一个孤立点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf06f3e-cbcc-4b3b-866f-0209a9336d2c",
   "metadata": {},
   "source": [
    "![图 1](StudyLocalOutlierFactor/8b58e902b7e799ce28c6da525be8ebfbbc23ef767a1f564df188a4167da32fec.png)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda85fc-b115-4bfb-8802-a6185611ef12",
   "metadata": {},
   "source": [
    "如图所示，越孤立的点，其LOF越高，我们只要算出来LOF的值，再给个阈值就能把所有的孤立点给挑出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973b8689-f546-4d74-9155-4ac6400d4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from Common.DataCenter import data_center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803dfda0-0dfd-492a-976d-35b6d638f5d7",
   "metadata": {},
   "source": [
    "现在构造一部分数据点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e39d889-e881-4896-8ceb-a9080ca47c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1,  1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[-1.1], [0.2], [101.1], [0.3]]\n",
    "clf = LocalOutlierFactor(n_neighbors=2)\n",
    "clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0042e-4ed4-419d-9228-c9e872ce3d57",
   "metadata": {},
   "source": [
    "我们能看得出来，最大的那个101.1被认为是outlier，看起来很不错，现在使用我们的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7df9a971-886b-43fd-ad3e-d60c5e22e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Common.DataCenter import data_center\n",
    "from Common.preprocessor import normalize_preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6cee362-def9-4ada-a6d1-c0acdea06987",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distribution = None\n",
    "#Each entry: source type and (size, distribution)\n",
    "noisy_set_sizes = {\n",
    "    'mislabeled' : (8000, None),                   # max size: 15000\n",
    "    'irrelevant' : (2000, [0.25,0.25,0.25,0.25]),  # max size: 34259\n",
    "    'translated' : (2000, \"reserve_labels\"),       # max size: 5000\n",
    "}\n",
    "dc = data_center(\"twitter_sentiment_data_clean.csv\", train_size = 20000, test_size = 4000, validation_size = 1000,\n",
    "                 noisy_size = noisy_set_sizes)\n",
    "\n",
    "trainDF = dc.get_train_with_noisy_df(4000, 1000, train_distribution)\n",
    "trainDF.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72373d01-4d7c-4f82-a99b-27640e00aa34",
   "metadata": {},
   "source": [
    "下面进行去噪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee29105-07c2-43ae-9f9f-f48c43896166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter: original X of training set and test set\n",
    "# return:  vectorised X of training set and test set\n",
    "def text_preprocessing(X_train):\n",
    "    # preprocessing with traditional NLP methodology\n",
    "    X_train_normalized = normalize_preprocessing(X_train)\n",
    "    \n",
    "    # vectorization\n",
    "    # Convert texts to vectors by TFIDF\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "    X_train_vec  = vectorizer.fit_transform(X_train_normalized)\n",
    "    return X_train_vec\n",
    "# parameter: original y of training set, original y of test set\n",
    "# return:  encoded y of training set and test set\n",
    "def one_hot_encoding(y_train):\n",
    "    mlb          = MultiLabelBinarizer()\n",
    "    y_train_vec  = mlb.fit_transform(map(str, y_train))\n",
    "    return y_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e827b9f-47c3-46fc-90f6-734ba1b523d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data_center.Xy(trainDF)\n",
    "X_train_vec = text_preprocessing(X_train)\n",
    "y_train_vec = one_hot_encoding(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa994587-bd53-4cb6-b516-92df2e16c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LocalOutlierFactor(n_neighbors=2)\n",
    "label_result = clf.fit_predict(X_train_vec)\n",
    "label_result = np.array(label_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "877f3dd0-848e-43ad-a86e-30482ffa2737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_result[label_result == -1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf1e4e1-cf65-44c4-9cac-fa17de5d21e4",
   "metadata": {},
   "source": [
    "这个方法比EllipticalEnvelope好多了，至少不要求dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24d6bc-bed8-46e6-a42b-2fd83430d376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
