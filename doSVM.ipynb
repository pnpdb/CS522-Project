{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adcf3ad4",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# SVM - Climate Sentiment Multiclass Classification\n",
    "## CS522 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69b1f5b",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Dataset: \n",
    "https://www.kaggle.com/code/luiskalckstein/climate-sentiment-multiclass-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c742fe8-1acf-4783-8888-6a5ff744ca05",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803ba8ae-b587-413e-b8fd-48aa4c7563c4",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from Common.DataCenter import data_center\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# When use pre-trained model\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "# When use climate corpus trained model\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b5697-fc30-4cdf-9a57-b7a11f8446a8",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8bc1977-fa6e-4cce-b2c3-a791cd7e7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the method of vectorization\n",
    "# enumeration: word2vec, tfidf\n",
    "vectorization_method = 'tfidf'\n",
    "\n",
    "# Select the corpus of word2vec\n",
    "# enumeration: google, glove, climate\n",
    "corpus_type = 'climate'\n",
    "\n",
    "# Set the length of vector\n",
    "# NOTICE: the dimension of vector must corresponding to the pre-trained model\n",
    "vector_size = 300\n",
    "\n",
    "# Choose vectorization method and pre-trained model\n",
    "# Climate Change Tweets, GoogleNews, Twitter\n",
    "# NOTICE: the dimension of selected file must corresponding to the vector_size\n",
    "if vectorization_method == 'word2vec':\n",
    "    if corpus_type == 'climate':\n",
    "        # Load climate pre-trained word2vec model with 300,100,50 dimension\n",
    "        wv_model_name = 'climates.41k.300d.bin'\n",
    "        wv_model = Word2Vec.load(os.path.join('.', 'models', wv_model_name))\n",
    "    elif corpus_type == 'google':\n",
    "        # Load GoogleNews pre-trained word2vec model with 300 dimension\n",
    "        wv_model_name = 'GoogleNews-vectors-negative300.bin'\n",
    "        wv_model = KeyedVectors.load_word2vec_format(os.path.join('.', 'models', wv_model_name), binary=True)\n",
    "    elif corpus_type == 'glove':\n",
    "        # Load twitter pre-trained glove model with 200,100,50,25 dimension\n",
    "        wv_model_name = 'glove.twitter.27B.200d.w2v.txt'\n",
    "        wv_model = KeyedVectors.load_word2vec_format(os.path.join('.', 'glove', wv_model_name), binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea169203",
   "metadata": {
    "pycharm": {
     "metadata": false
    },
    "tags": []
   },
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea1c9de-20d4-4f1d-bf60-ac7125cc292e",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameter: list of tweet messages\n",
    "# return: normalization of pre-processing\n",
    "def normalize_preprocessing(data, split=False):\n",
    "    \n",
    "    messages = []\n",
    "    \n",
    "    # Traversal the message list\n",
    "    for i in range(len(data)):\n",
    "        # Lower case\n",
    "        message         = data[i].lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        for c in string.punctuation:\n",
    "            message         = message.replace(c, ' ')\n",
    "            \n",
    "        # Tokenize\n",
    "        message         = nltk.word_tokenize(message)\n",
    "        \n",
    "        # Comment this part for no working\n",
    "        ## Remove stop words\n",
    "        # message_filtered = [w for w in message if w not in stopwords.words('english')]\n",
    "        \n",
    "        # Comment this part for no working\n",
    "        ## Only keep Noun and specified POS\n",
    "        # message_refiltered = nltk.pos_tag(message_filtered)\n",
    "        # message_filtered = [w for w, pos in message_refiltered if pos.startswith('NN')]\n",
    "        \n",
    "        # Stemming\n",
    "        ps               = PorterStemmer()\n",
    "        # message_filtered = [ps.stem(w) for w in message_filtered]\n",
    "        message_filtered = [ps.stem(w) for w in message]\n",
    "        \n",
    "        # Re-Combinate\n",
    "        if split == True:\n",
    "            message      = message_filtered\n",
    "        else:\n",
    "            message      = \" \".join(message_filtered)\n",
    "\n",
    "        messages.append(message)\n",
    "        \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0071317f-5a3e-4086-b99f-d7e4056bbf1b",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameter: list of normalized tweets\n",
    "# return: embedding sentence vector with mean of Word2Vec\n",
    "def w2vVectorizer(messages, size=vector_size):\n",
    "    \n",
    "    # Convert to vectors with pre-trained word2vec\n",
    "    length = len(messages)\n",
    "    array = np.zeros((length, size))\n",
    "    message_count = 0\n",
    "    \n",
    "    for message in messages:\n",
    "        word_count = 0\n",
    "        for word in message:\n",
    "            try:\n",
    "                if corpus_type == 'climate':\n",
    "                    # When vector of word2vec\n",
    "                    array[message_count,] += wv_model.wv[word]\n",
    "                elif corpus_type == 'google':\n",
    "                    # When keyedvector of word2vec\n",
    "                    array[message_count,] += wv_model[word]\n",
    "                elif corpus_type == 'glove':\n",
    "                    # When keyedvector of word2vec\n",
    "                    array[message_count,] += wv_model[word]\n",
    "                    \n",
    "                word_count += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "                \n",
    "        if word_count != 0:\n",
    "            array[message_count,] /= word_count\n",
    "            \n",
    "        message_count +=1\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be3703ac-81e8-4ba0-848f-b9de87ac0543",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameter: original X of training set and test set\n",
    "# return:  vectorised X of training set and test set\n",
    "def text_preprocessing(X_train, X_test):\n",
    "    \n",
    "    # preprocessing with traditional NLP methodology\n",
    "    split   = False\n",
    "    if vectorization_method == 'word2vec':\n",
    "        split = True\n",
    "    X_train_normalized = normalize_preprocessing(X_train, split)\n",
    "    X_test_normalized  = normalize_preprocessing(X_test, split)\n",
    "    \n",
    "    # vectorization\n",
    "    if vectorization_method == 'word2vec':\n",
    "        # Convert texts to vectors by Word2Vec\n",
    "        X_train_vec  = w2vVectorizer(X_train_normalized)\n",
    "        X_test_vec   = w2vVectorizer(X_test_normalized)\n",
    "    else:\n",
    "        # Convert texts to vectors by TFIDF\n",
    "        vectorizer   = TfidfVectorizer()\n",
    "        X_train_vec  = vectorizer.fit_transform(X_train_normalized)\n",
    "        X_test_vec   = vectorizer.transform(X_test_normalized)\n",
    "      \n",
    "    return X_train_vec, X_test_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d7615",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### One-hot encoding, convert the labels to vectors (4 x 1) each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a4ab11",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameter: original y of training set, original y of test set\n",
    "# return:  encoded y of training set and test set\n",
    "def one_hot_encoding(y_train, y_test):\n",
    "    mlb          = MultiLabelBinarizer()\n",
    "    y_train_vec  = mlb.fit_transform(map(str, y_train))\n",
    "    y_test_vec   = mlb.transform(map(str, y_test))\n",
    "    return y_train_vec, y_test_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eaf9b6",
   "metadata": {
    "pycharm": {
     "metadata": false
    },
    "tags": []
   },
   "source": [
    "### Run SVM and evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7660ca25",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameter:  vectorised X and encoded y of training set and test set\n",
    "def evaluate_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec):\n",
    "    # Run SVM - fit and predict\n",
    "    SVM             = OneVsRestClassifier(LinearSVC(), n_jobs=-1)\n",
    "    SVM.fit(X_train_vec, y_train_vec)\n",
    "    prediction      = SVM.predict(X_test_vec)\n",
    "\n",
    "    # Evaluate the results\n",
    "    macro_f1        = f1_score(y_test_vec, prediction, average='macro')\n",
    "    weighted_f1     = f1_score(y_test_vec, prediction, average='weighted')\n",
    "    macro_precision = precision_score(y_test_vec, prediction, average='macro')\n",
    "    macro_recall    = recall_score(y_test_vec, prediction, average='macro')\n",
    "\n",
    "    return macro_f1, weighted_f1, macro_precision, macro_recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0772c",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Do an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55d424b",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter: original X,y of training set and test set\n",
    "def do_experiment(X_train, y_train, X_test, y_test):\n",
    "    # Convert texts to vectors\n",
    "    X_train_vec, X_test_vec = text_preprocessing(X_train, X_test)\n",
    "    y_train_vec, y_test_vec = one_hot_encoding(y_train, y_test)\n",
    "    \n",
    "    # Run SVM and evaluate the results\n",
    "    macro_f1, weighted_f1, macro_precision, macro_recall = \\\n",
    "        evaluate_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec)\n",
    "\n",
    "    # Show the indicators\n",
    "    print(\" macro_f1: %.4f , weighted_f1: %.4f, macro_precision: %.4f, macro_recall: %.4f\" %\n",
    "          (macro_f1, weighted_f1, macro_precision, macro_recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000bb7a",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Main entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0d3ee",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Load the database and split it into training set, test set, noisy set, validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba163848",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################\n",
      "Total data size:  40908\n",
      "Total train data size:  19908\n",
      "Total test data size:  8000\n"
     ]
    }
   ],
   "source": [
    "dc = data_center(\"twitter_sentiment_data_clean.csv\", test_size=8000, noisy_size=8000, validation_size=5000)\n",
    "\n",
    "print(\"####################################################\")\n",
    "print(\"Total data size: \",       dc.get_len())\n",
    "print(\"Total train data size: \", dc.get_train_len())\n",
    "print(\"Total test data size: \",  dc.get_test_len())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a6679",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Get the test set for evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31905bce",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test = dc.get_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea92f2",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Run experiments with different training sets, and use the same test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f512a38d",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Training set size: 2000 samples (10.0%): \n",
      " macro_f1: 0.4725 , weighted_f1: 0.5717, macro_precision: 0.6499, macro_recall: 0.4261\n",
      "Training set size: 2500 samples (12.6%): \n",
      " macro_f1: 0.4917 , weighted_f1: 0.5840, macro_precision: 0.6557, macro_recall: 0.4438\n",
      "Training set size: 4000 samples (20.1%): \n",
      " macro_f1: 0.5298 , weighted_f1: 0.6135, macro_precision: 0.6753, macro_recall: 0.4813\n",
      "Training set size: 5000 samples (25.1%): \n",
      " macro_f1: 0.5446 , weighted_f1: 0.6239, macro_precision: 0.6826, macro_recall: 0.4941\n",
      "Training set size: 7500 samples (37.7%): \n",
      " macro_f1: 0.5661 , weighted_f1: 0.6387, macro_precision: 0.6876, macro_recall: 0.5176\n",
      "Training set size: 10000 samples (50.2%): \n",
      " macro_f1: 0.5856 , weighted_f1: 0.6531, macro_precision: 0.7033, macro_recall: 0.5374\n",
      "-----------------------------------------------\n",
      "Noisy training set size: 2499 samples (2000 original, 500 noisy)\n",
      " macro_f1: 0.4068 , weighted_f1: 0.5031, macro_precision: 0.5855, macro_recall: 0.3392\n",
      "Noisy training set size: 5000 samples (4000 original, 1000 noisy)\n",
      " macro_f1: 0.4603 , weighted_f1: 0.5463, macro_precision: 0.6154, macro_recall: 0.3919\n",
      "Noisy training set size: 10000 samples (7500 original, 2500 noisy)\n",
      " macro_f1: 0.4749 , weighted_f1: 0.5491, macro_precision: 0.6266, macro_recall: 0.3955\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------\")\n",
    "for size in [2000, 2500, 4000, 5000, 7500, 10000]:\n",
    "    # Get a training set without noisy data\n",
    "    X_train, y_train = dc.get_train(size)\n",
    "    print(\"Training set size: %d samples (%.1f%%): \" % (len(X_train), len(y_train)/dc.get_train_len()*100))\n",
    "\n",
    "    # Do an experiment\n",
    "    do_experiment(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"-----------------------------------------------\")\n",
    "for size in [(2000, 500), (4000, 1000), (7500, 2500)]:\n",
    "    # Get a noisy training set\n",
    "    X_train, y_train = dc.get_train_with_noisy(size[0], size[1])\n",
    "    print(\"Noisy training set size: %d samples (%d original, %d noisy)\" % (len(y_train), size[0], size[1]))\n",
    "\n",
    "    # Do an experiment\n",
    "    do_experiment(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e38ecd",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
