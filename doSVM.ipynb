{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "# SVM - Climate Sentiment Multiclass Classification\n",
        "## CS522 Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### Dataset: \n",
        "https://www.kaggle.com/code/luiskalckstein/climate-sentiment-multiclass-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom Common.DataCenter import data_center\nfrom Common.preprocessor import normalize_preprocessing\nfrom Common.UtilFuncs import print_evaluation, print_distribution\n\n%matplotlib inline"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        },
        "tags": []
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# parameter: original X of training set and test set\n",
        "# return:  vectorised X of training set and test set\n",
        "def text_preprocessing(X_train, X_test):\n",
        "    \n",
        "    # preprocessing with traditional NLP methodology\n",
        "    X_train_normalized \u003d normalize_preprocessing(X_train)\n",
        "    X_test_normalized  \u003d normalize_preprocessing(X_test)\n",
        "    \n",
        "    # vectorization\n",
        "    # Convert texts to vectors by TFIDF\n",
        "    vectorizer \u003d TfidfVectorizer(ngram_range\u003d(1,2))\n",
        "    X_train_vec  \u003d vectorizer.fit_transform(X_train_normalized)\n",
        "    X_test_vec   \u003d vectorizer.transform(X_test_normalized)\n",
        "      \n",
        "    return X_train_vec, X_test_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### One-hot encoding, convert the labels to vectors (4 x 1) each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# parameter: original y of training set, original y of test set\n",
        "# return:  encoded y of training set and test set\n",
        "def one_hot_encoding(y_train, y_test):\n",
        "    mlb          \u003d MultiLabelBinarizer()\n",
        "    y_train_vec  \u003d mlb.fit_transform(map(str, y_train))\n",
        "    y_test_vec   \u003d mlb.transform(map(str, y_test))\n",
        "    return y_train_vec, y_test_vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        },
        "tags": []
      },
      "source": [
        "### Run SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# parameter:  vectorised X and encoded y of training set and test set\n",
        "def run_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec):\n",
        "    # Run SVM - fit and predict\n",
        "    SVM             \u003d OneVsRestClassifier(LinearSVC(dual\u003dFalse, class_weight\u003d\u0027balanced\u0027), n_jobs\u003d-1)\n",
        "    SVM.fit(X_train_vec, y_train_vec)\n",
        "    y_pred          \u003d SVM.predict(X_test_vec)\n",
        "    return  y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### Do an experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Parameter: original X,y of training set and test set\n",
        "def do_experiment(X_train, y_train, X_test, y_test):\n",
        "    # Convert texts to vectors\n",
        "    X_train_vec, X_test_vec \u003d text_preprocessing(X_train, X_test)\n",
        "    y_train_vec, y_test_vec \u003d one_hot_encoding(y_train, y_test)\n",
        "\n",
        "    # Run SVM and evaluate the results\n",
        "    y_pred \u003d run_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec)\n",
        "\n",
        "    # Print the evaluation\n",
        "    print_evaluation(y_test_vec, y_pred, labels\u003d[0,1,2,3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": "### Main entry\n**Load the database and split it into training set, test set, noisy set, validation set**"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "####################################################\n",
            "Total data size:  40908\n",
            "Total train data size:  33908\n",
            "Total test data size:  4000\n"
          ]
        }
      ],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "source": "# The size of the noise sources",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\nnoisy_set_sizes \u003d {\n    \u0027mislabeled\u0027 : 5000,   # max size: 15000\n    \u0027irrelevant\u0027 : 5000,   # max size: 34259\n    \u0027translated\u0027 : 5000,   # max size: 5000\n}\n\n# Load the database and split it into training set, test set, noisy set, validation set\ndc \u003d data_center(\"twitter_sentiment_data_clean.csv\", test_size \u003d 4000, validation_size \u003d 1000,\n                 noisy_size \u003d noisy_set_sizes[\u0027mislabeled\u0027])\n\nprint(\"####################################################\")\nprint(\"Total data size: \",       dc.get_len())\nprint(\"Total train data size: \", dc.get_train_len())\nprint(\"Total test data size: \",  dc.get_test_len())",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": "**Get the test set for evaluation**"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "X_test, y_test \u003d dc.get_test()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "**Set distributions of training set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# distribution of training set\n",
        "train_distribution \u003d None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "**Prepare the noisy set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Noisy set size is 3000\n",
            "6000 noisy samples added\n",
            "6000 noisy samples added\n",
            "Noisy set new size is 15000\n"
          ]
        }
      ],
      "source": "lstNoisyInfo \u003d [(\"mislabeled\",dc.get_noisy_len())]\nprint(\"Noisy set size is %d\"                % dc.get_noisy_len())\n\n# add the external noisy data (irrelevant texts)\n# distribution of irrelevant noisy\nirrelevant_noisy_distribution \u003d [0.25, 0.25, 0.25, 0.25]    # None, if use the distribution of original set\nadded_size \u003d dc.add_noisy(noisy_source\u003d\"irrelevant\", distribution \u003d irrelevant_noisy_distribution,\n                          size \u003d noisy_set_sizes[\u0027irrelevant\u0027])\nprint(\"%d noisy samples added\" % added_size)\nlstNoisyInfo.append((\"irrelevant\",added_size))\n\n# add the external noisy data (translated texts). use the labels of each noisy data\nadded_size \u003d dc.add_noisy(noisy_source\u003d\"translated\", distribution \u003d \"reserve_labels\", \n                          size \u003d noisy_set_sizes[\u0027translated\u0027])\nprint(\"%d noisy samples added\" % added_size)\nlstNoisyInfo.append((\"translated\",added_size))\n\nprint(\"Noisy set new size is %d\"                % dc.get_noisy_len())\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": "**Run experiments with different training sets, and use the same test set.**"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- No noisy training sets ----------\n",
            "* Training set size: 2000 samples: \n",
            "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
            "  f1 of classes: [0.317, 0.325, 0.737, 0.679]\n",
            "  micro_f1: 0.640 , macro_f1: 0.514 , weighted_f1: 0.609, macro_precision: 0.609, macro_recall: 0.504\n",
            "* Training set size: 4000 samples: \n",
            "  Sentiment distribution: 9.4%, 50.2%, 18.3%, 22.1%\n",
            "  f1 of classes: [0.367, 0.387, 0.75, 0.688]\n",
            "  micro_f1: 0.659 , macro_f1: 0.548 , weighted_f1: 0.634, macro_precision: 0.629, macro_recall: 0.538\n",
            "* Training set size: 5000 samples: \n",
            "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
            "  f1 of classes: [0.375, 0.394, 0.752, 0.702]\n",
            "  micro_f1: 0.664 , macro_f1: 0.556 , weighted_f1: 0.640, macro_precision: 0.617, macro_recall: 0.551\n",
            "* Training set size: 8000 samples: \n",
            "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
            "  f1 of classes: [0.447, 0.445, 0.773, 0.724]\n",
            "  micro_f1: 0.690 , macro_f1: 0.597 , weighted_f1: 0.671, macro_precision: 0.642, macro_recall: 0.595\n",
            "* Training set size: 10000 samples: \n",
            "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
            "  f1 of classes: [0.488, 0.459, 0.773, 0.731]\n",
            "  micro_f1: 0.696 , macro_f1: 0.613 , weighted_f1: 0.680, macro_precision: 0.650, macro_recall: 0.612\n",
            "* Training set size: 15000 samples: \n",
            "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
            "  f1 of classes: [0.53, 0.486, 0.785, 0.752]\n",
            "  micro_f1: 0.713 , macro_f1: 0.638 , weighted_f1: 0.699, macro_precision: 0.667, macro_recall: 0.640\n",
            "* Training set size: 20000 samples: \n",
            "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
            "  f1 of classes: [0.562, 0.512, 0.796, 0.759]\n",
            "  micro_f1: 0.725 , macro_f1: 0.657 , weighted_f1: 0.714, macro_precision: 0.678, macro_recall: 0.660\n",
            "-------------- Noisy training sets -------------\n",
            "The proportions of the noise sources [\u0027mislabeled\u0027, \u0027irrelevant\u0027, \u0027translated\u0027]:  [20.0, 40.0, 40.0]\n",
            "* Noisy training set size: 5000 samples (4000 original, 1000 noisy)\n",
            "  Sentiment distribution: 12.2%, 19.5%, 45.9%, 22.3%\n",
            "  f1 of classes: [0.381, 0.401, 0.74, 0.682]\n",
            "  micro_f1: 0.651 , macro_f1: 0.551 , weighted_f1: 0.631, macro_precision: 0.629, macro_recall: 0.528\n",
            "* Noisy training set size: 11000 samples (8000 original, 3000 noisy)\n",
            "  Sentiment distribution: 13.1%, 19.7%, 44.5%, 22.8%\n",
            "  f1 of classes: [0.448, 0.477, 0.752, 0.704]\n",
            "  micro_f1: 0.673 , macro_f1: 0.595 , weighted_f1: 0.663, macro_precision: 0.637, macro_recall: 0.574\n",
            "* Noisy training set size: 20000 samples (15000 original, 5000 noisy)\n",
            "  Sentiment distribution: 12.8%, 19.7%, 44.9%, 22.6%\n",
            "  f1 of classes: [0.542, 0.492, 0.771, 0.729]\n",
            "  micro_f1: 0.696 , macro_f1: 0.634 , weighted_f1: 0.689, macro_precision: 0.656, macro_recall: 0.621\n"
          ]
        }
      ],
      "source": "print(\"-------------- No noisy training sets ----------\")\nfor size in [2000, 4000, 5000, 8000, 10000, 15000, 20000]:\n    # Get a training set without noisy data\n    X_train, y_train \u003d dc.get_train(size, train_distribution)\n    print(\"* Training set size: %d samples: \" % (len(X_train)))\n    print_distribution(\"  Sentiment distribution\", y_train)\n\n    # Do an experiment\n    do_experiment(X_train, y_train, X_test, y_test)\n\nprint(\"-------------- Noisy training sets -------------\")\nprint(\"The proportions of the noise sources %s: \" % [x[0] for x in lstNoisyInfo],\n      [round(x[1]*100/dc.get_noisy_len(),1) for x in lstNoisyInfo])\nfor size in [(4000, 1000), (8000, 3000), (15000, 5000)]:\n    # Get a noisy training set\n    X_train, y_train \u003d dc.get_train_with_noisy(size[0], size[1], train_distribution)\n    print(\"* Noisy training set size: %d samples (%d original, %d noisy)\" % (len(y_train), size[0], size[1]))\n    print_distribution(\"  Sentiment distribution\", y_train)\n\n    # Do an experiment\n    do_experiment(X_train, y_train, X_test, y_test)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "stem_cell": {
      "cell_type": "raw",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}