{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# SVM - Climate Sentiment Multiclass Classification\n",
    "## CS522 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Dataset: \n",
    "https://www.kaggle.com/code/luiskalckstein/climate-sentiment-multiclass-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from Common.DataCenter import data_center\n",
    "from Common.preprocessor import normalize_preprocessing\n",
    "from Common.UtilFuncs import print_evaluation, print_distribution\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    },
    "tags": []
   },
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameter: original X of training set and test set\n",
    "# return:  vectorised X of training set and test set\n",
    "def text_preprocessing(X_train, X_test):\n",
    "    \n",
    "    # preprocessing with traditional NLP methodology\n",
    "    X_train_normalized = normalize_preprocessing(X_train)\n",
    "    X_test_normalized  = normalize_preprocessing(X_test)\n",
    "    \n",
    "    # vectorization\n",
    "    # Convert texts to vectors by TFIDF\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "    X_train_vec  = vectorizer.fit_transform(X_train_normalized)\n",
    "    X_test_vec   = vectorizer.transform(X_test_normalized)\n",
    "      \n",
    "    return X_train_vec, X_test_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### One-hot encoding, convert the labels to vectors (4 x 1) each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameter: original y of training set, original y of test set\n",
    "# return:  encoded y of training set and test set\n",
    "def one_hot_encoding(y_train, y_test):\n",
    "    mlb          = MultiLabelBinarizer()\n",
    "    y_train_vec  = mlb.fit_transform(map(str, y_train))\n",
    "    y_test_vec   = mlb.transform(map(str, y_test))\n",
    "    return y_train_vec, y_test_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    },
    "tags": []
   },
   "source": [
    "### Run SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameter:  vectorised X and encoded y of training set and test set\n",
    "def run_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec):\n",
    "    # Run SVM - fit and predict\n",
    "    SVM             = OneVsRestClassifier(LinearSVC(dual=False, class_weight='balanced'), n_jobs=-1)\n",
    "    SVM.fit(X_train_vec, y_train_vec)\n",
    "    y_pred          = SVM.predict(X_test_vec)\n",
    "    return  y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Do an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter: original X,y of training set and test set\n",
    "def do_experiment(X_train, y_train, X_test, y_test):\n",
    "    # Convert texts to vectors\n",
    "    X_train_vec, X_test_vec = text_preprocessing(X_train, X_test)\n",
    "    y_train_vec, y_test_vec = one_hot_encoding(y_train, y_test)\n",
    "\n",
    "    # Run SVM and evaluate the results\n",
    "    y_pred = run_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec)\n",
    "\n",
    "    # Print the evaluation\n",
    "    print_evaluation(y_test_vec, y_pred, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Main entry\n",
    "**Load the database and split it into training set, test set, noisy set, validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# The size of the noise sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################\n",
      "Total data size:  40908\n",
      "Total train data size:  20000\n",
      "Total test data size:  4000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noisy_set_sizes = {\n",
    "    'mislabeled' : 5000,   # max size: 15000\n",
    "    'irrelevant' : 5000,   # max size: 34259\n",
    "    'translated' : 5000,   # max size: 5000\n",
    "}\n",
    "\n",
    "# Load the database and split it into training set, test set, noisy set, validation set\n",
    "dc = data_center(\"twitter_sentiment_data_clean.csv\", train_size = 20000, test_size = 4000, validation_size = 1000,\n",
    "                 noisy_size = noisy_set_sizes['mislabeled'] if 'mislabeled' in noisy_set_sizes.keys() else 0)\n",
    "\n",
    "print(\"####################################################\")\n",
    "print(\"Total data size: \",       dc.get_len())\n",
    "print(\"Total train data size: \", dc.get_train_len())\n",
    "print(\"Total test data size: \",  dc.get_test_len())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Get the test set for evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test = dc.get_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Set distributions of training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# distribution of training set\n",
    "train_distribution = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Prepare the noisy set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 noisy samples of 'mislabeled' added\n",
      "5000 noisy samples of 'irrelevant' added\n",
      "5000 noisy samples of 'translated' added\n",
      "The total size of noisy data is 15000\n"
     ]
    }
   ],
   "source": [
    "lstNoisyInfo    = []\n",
    "if 'mislabeled' in noisy_set_sizes.keys() and noisy_set_sizes['mislabeled'] > 0:\n",
    "    lstNoisyInfo.append((\"mislabeled\",dc.get_noisy_len()))\n",
    "    print(\"%d noisy samples of '%s' added\" % (dc.get_noisy_len(), 'mislabeled'))\n",
    "\n",
    "# add the external noisy data (irrelevant texts)\n",
    "if 'irrelevant' in noisy_set_sizes.keys() and noisy_set_sizes['irrelevant'] > 0:\n",
    "    # distribution of irrelevant noisy\n",
    "    irrelevant_noisy_distribution = [0.25, 0.25, 0.25, 0.25]    # None, if use the distribution of original set\n",
    "    added_size = dc.add_noisy(noisy_source=\"irrelevant\", distribution = irrelevant_noisy_distribution,\n",
    "                              size = noisy_set_sizes['irrelevant'])\n",
    "    print(\"%d noisy samples of '%s' added\"  % (added_size, 'irrelevant'))\n",
    "    lstNoisyInfo.append((\"irrelevant\",added_size))\n",
    "\n",
    "# add the external noisy data (translated texts). use the labels of each noisy data\n",
    "if 'translated' in noisy_set_sizes.keys() and noisy_set_sizes['translated'] > 0:\n",
    "    added_size = dc.add_noisy(noisy_source=\"translated\", distribution = \"reserve_labels\",\n",
    "                              size = noisy_set_sizes['translated'])\n",
    "    print(\"%d noisy samples of '%s' added\"  % (added_size, 'translated'))\n",
    "    lstNoisyInfo.append((\"translated\",added_size))\n",
    "\n",
    "print(\"The total size of noisy data is %d\"                % dc.get_noisy_len())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Run experiments with different training sets, and use the same test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- No noisy training sets ----------\n",
      "* Training set size: 2000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.349, 0.315, 0.711, 0.678]\n",
      "  micro_f1: 0.626 , macro_f1: 0.513 , weighted_f1: 0.597, macro_precision: 0.592, macro_recall: 0.504\n",
      "* Training set size: 4000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.442, 0.371, 0.741, 0.695]\n",
      "  micro_f1: 0.657 , macro_f1: 0.562 , weighted_f1: 0.635, macro_precision: 0.623, macro_recall: 0.554\n",
      "* Training set size: 5000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.466, 0.4, 0.746, 0.709]\n",
      "  micro_f1: 0.668 , macro_f1: 0.580 , weighted_f1: 0.648, macro_precision: 0.636, macro_recall: 0.570\n",
      "* Training set size: 8000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.521, 0.42, 0.758, 0.719]\n",
      "  micro_f1: 0.682 , macro_f1: 0.604 , weighted_f1: 0.665, macro_precision: 0.648, macro_recall: 0.598\n",
      "* Training set size: 10000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.534, 0.431, 0.765, 0.727]\n",
      "  micro_f1: 0.689 , macro_f1: 0.614 , weighted_f1: 0.674, macro_precision: 0.645, macro_recall: 0.613\n",
      "* Training set size: 15000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.567, 0.452, 0.774, 0.74]\n",
      "  micro_f1: 0.702 , macro_f1: 0.633 , weighted_f1: 0.688, macro_precision: 0.657, macro_recall: 0.637\n",
      "* Training set size: 20000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.599, 0.483, 0.788, 0.746]\n",
      "  micro_f1: 0.716 , macro_f1: 0.654 , weighted_f1: 0.705, macro_precision: 0.668, macro_recall: 0.660\n",
      "-------------- Noisy training sets -------------\n",
      "The proportions of the noise sources ['mislabeled', 'irrelevant', 'translated']:  [33.3, 33.3, 33.3]\n",
      "* Noisy training set size: 5000 samples (4000 original, 1000 noisy)\n",
      "  Sentiment distribution: 11.7%, 19.2%, 46.2%, 22.9%\n",
      "  f1 of classes: [0.415, 0.361, 0.721, 0.684]\n",
      "  micro_f1: 0.633 , macro_f1: 0.545 , weighted_f1: 0.618, macro_precision: 0.590, macro_recall: 0.524\n",
      "* Noisy training set size: 10000 samples (8000 original, 2000 noisy)\n",
      "  Sentiment distribution: 11.8%, 19.7%, 45.9%, 22.6%\n",
      "  f1 of classes: [0.509, 0.431, 0.735, 0.704]\n",
      "  micro_f1: 0.660 , macro_f1: 0.595 , weighted_f1: 0.651, macro_precision: 0.626, macro_recall: 0.576\n",
      "* Noisy training set size: 20000 samples (15000 original, 5000 noisy)\n",
      "  Sentiment distribution: 12.8%, 19.6%, 44.5%, 23.1%\n",
      "  f1 of classes: [0.545, 0.464, 0.75, 0.717]\n",
      "  micro_f1: 0.676 , macro_f1: 0.619 , weighted_f1: 0.671, macro_precision: 0.638, macro_recall: 0.606\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------- No noisy training sets ----------\")\n",
    "for size in [2000, 4000, 5000, 8000, 10000, 15000, 20000]:\n",
    "    # Get a training set without noisy data\n",
    "    X_train, y_train = dc.get_train(size, train_distribution)\n",
    "    print(\"* Training set size: %d samples: \" % (len(X_train)))\n",
    "    print_distribution(\"  Sentiment distribution\", y_train)\n",
    "\n",
    "    # Do an experiment\n",
    "    do_experiment(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"-------------- Noisy training sets -------------\")\n",
    "print(\"The proportions of the noise sources %s: \" % [x[0] for x in lstNoisyInfo],\n",
    "      [round(x[1]*100/dc.get_noisy_len(),1) for x in lstNoisyInfo])\n",
    "for size in [(4000, 1000), (8000, 2000), (15000, 5000)]:\n",
    "    # Get a noisy training set\n",
    "    X_train, y_train = dc.get_train_with_noisy(size[0], size[1], train_distribution)\n",
    "    print(\"* Noisy training set size: %d samples (%d original, %d noisy)\" % (len(y_train), size[0], size[1]))\n",
    "    print_distribution(\"  Sentiment distribution\", y_train)\n",
    "\n",
    "    # Do an experiment\n",
    "    do_experiment(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
