{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "adcf3ad4",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "# SVM - Climate Sentiment Multiclass Classification\n",
        "## CS522 Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f69b1f5b",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### Dataset: \n",
        "https://www.kaggle.com/code/luiskalckstein/climate-sentiment-multiclass-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c742fe8-1acf-4783-8888-6a5ff744ca05",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "803ba8ae-b587-413e-b8fd-48aa4c7563c4",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.svm import LinearSVC\nfrom Common.DataCenter import data_center\nfrom Common.preprocessor import normalize_preprocessing\nfrom Common.UtilFuncs import print_evaluation\n\n%matplotlib inline"
    },
    {
      "cell_type": "markdown",
      "id": "ea169203",
      "metadata": {
        "pycharm": {
          "metadata": false
        },
        "tags": []
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "be3703ac-81e8-4ba0-848f-b9de87ac0543",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "# parameter: original X of training set and test set\n# return:  vectorised X of training set and test set\ndef text_preprocessing(X_train, X_test):\n    \n    # preprocessing with traditional NLP methodology\n    X_train_normalized \u003d normalize_preprocessing(X_train)\n    X_test_normalized  \u003d normalize_preprocessing(X_test)\n    \n    # vectorization\n    # Convert texts to vectors by TFIDF\n    vectorizer \u003d TfidfVectorizer(ngram_range\u003d(1,2))\n    X_train_vec  \u003d vectorizer.fit_transform(X_train_normalized)\n    X_test_vec   \u003d vectorizer.transform(X_test_normalized)\n      \n    return X_train_vec, X_test_vec"
    },
    {
      "cell_type": "markdown",
      "id": "7b4d7615",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### One-hot encoding, convert the labels to vectors (4 x 1) each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e3a4ab11",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# parameter: original y of training set, original y of test set\n",
        "# return:  encoded y of training set and test set\n",
        "def one_hot_encoding(y_train, y_test):\n",
        "    mlb          \u003d MultiLabelBinarizer()\n",
        "    y_train_vec  \u003d mlb.fit_transform(map(str, y_train))\n",
        "    y_test_vec   \u003d mlb.transform(map(str, y_test))\n",
        "    return y_train_vec, y_test_vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2eaf9b6",
      "metadata": {
        "pycharm": {
          "metadata": false
        },
        "tags": []
      },
      "source": "### Run SVM"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7660ca25",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "# parameter:  vectorised X and encoded y of training set and test set\ndef run_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec):\n    # Run SVM - fit and predict\n    SVM             \u003d OneVsRestClassifier(LinearSVC(dual\u003dFalse, class_weight\u003d\u0027balanced\u0027), n_jobs\u003d-1)\n    SVM.fit(X_train_vec, y_train_vec)\n    y_pred          \u003d SVM.predict(X_test_vec)\n    return  y_pred\n"
    },
    {
      "cell_type": "markdown",
      "id": "32c0772c",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### Do an experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c55d424b",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "# Parameter: original X,y of training set and test set\ndef do_experiment(X_train, y_train, X_test, y_test):\n    # Convert texts to vectors\n    X_train_vec, X_test_vec \u003d text_preprocessing(X_train, X_test)\n    y_train_vec, y_test_vec \u003d one_hot_encoding(y_train, y_test)\n\n    # Run SVM and evaluate the results\n    y_pred \u003d run_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec)\n\n    # Print the evaluation\n    print_evaluation(y_test_vec, y_pred, labels\u003d[0,1,2,3])\n"
    },
    {
      "cell_type": "markdown",
      "source": "### Print the distribution of labels",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def print_distribution(hint, y):\n    df \u003d data_center.df((y, y))\n    c \u003d df[\u0027sentiment\u0027].value_counts(sort \u003d False)\n    l \u003d len(df)\n    print(\"%s: %s\" % (hint, (\"%.1f%%, \"*(len(c)-1)+\"%.1f%%\") % tuple([x*100/l for x in list(c)])))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "id": "7000bb7a",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": "### Main entry"
    },
    {
      "cell_type": "markdown",
      "id": "4ca0d3ee",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "**Load the database and split it into training set, test set, noisy set, validation set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ba163848",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "####################################################\n",
            "Total data size:  41033\n",
            "Total train data size:  20033\n",
            "Total test data size:  8000\n"
          ]
        }
      ],
      "source": "dc \u003d data_center(\"twitter_sentiment_data_clean.csv\", test_size\u003d4000, noisy_size\u003d3000, validation_size\u003d0)\n\nprint(\"####################################################\")\nprint(\"Total data size: \",       dc.get_len())\nprint(\"Total train data size: \", dc.get_train_len())\nprint(\"Total test data size: \",  dc.get_test_len())"
    },
    {
      "cell_type": "markdown",
      "id": "408a6679",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "**Get the test set for evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "31905bce",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "X_test, y_test \u003d dc.get_test()\n"
    },
    {
      "cell_type": "markdown",
      "source": "**Set distributions of training set.**",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# distribution of training set\ntrain_distribution \u003d None\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Prepare the noisy set.**",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# distribution of external noisy\nexternal_noisy_distribution \u003d [0.25, 0.25, 0.25, 0.25]\nlstNoisyInfo \u003d [(\"mislabeled\",dc.get_noisy_len())]\nprint(\"Noisy set size is %d\"                % dc.get_noisy_len())\n\n# add the external noisy data (irrelevant texts)\nadded_size \u003d dc.add_noisy(noisy_source\u003d\"irrelevant\",\n                          distribution \u003d external_noisy_distribution, size \u003d 6000) # max size: 34259\nprint(\"%d noisy samples added\" % added_size)\nlstNoisyInfo.append((\"irrelevant\",added_size))\n\n# add the external noisy data (translated texts)\nadded_size \u003d dc.add_noisy(noisy_source\u003d\"translated\",\n                          distribution \u003d \"reserve_labels\", size \u003d 6000) # max size: 6146\nprint(\"%d noisy samples added\" % added_size)\nlstNoisyInfo.append((\"translated\",added_size))\n\nprint(\"Noisy set new size is %d\"                % dc.get_noisy_len())\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "id": "4dea92f2",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": "**Run experiments with different training sets, and use the same test set.**"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f512a38d",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------\n",
            "Training set size: 1000 samples (5.0%): \n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "fit() got an unexpected keyword argument \u0027n_classes\u0027",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m\u003ccell line: 2\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining set size: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m samples (\u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(X_train), \u001b[38;5;28mlen\u001b[39m(y_train)\u001b[38;5;241m/\u001b[39mdc\u001b[38;5;241m.\u001b[39mget_train_len()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Do an experiment\u001b[39;00m\n\u001b[0;32m----\u003e 8\u001b[0m     \u001b[43mdo_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m500\u001b[39m), (\u001b[38;5;241m4000\u001b[39m, \u001b[38;5;241m1000\u001b[39m), (\u001b[38;5;241m7500\u001b[39m, \u001b[38;5;241m2500\u001b[39m)]:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Get a noisy training set\u001b[39;00m\n",
            "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mdo_experiment\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m      5\u001b[0m y_train_vec, y_test_vec \u001b[38;5;241m\u003d\u001b[39m one_hot_encoding(y_train, y_test)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Run SVM and evaluate the results\u001b[39;00m\n\u001b[1;32m      8\u001b[0m macro_f1, weighted_f1, macro_precision, macro_recall \u001b[38;5;241m\u003d\u001b[39m \\\n\u001b[0;32m----\u003e 9\u001b[0m     \u001b[43mevaluate_SVM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Show the indicators\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m macro_f1: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m , weighted_f1: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m, macro_precision: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m, macro_recall: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m     13\u001b[0m       (macro_f1, weighted_f1, macro_precision, macro_recall))\n",
            "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mevaluate_SVM\u001b[0;34m(X_train_vec, y_train_vec, X_test_vec, y_test_vec)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_SVM\u001b[39m(X_train_vec, y_train_vec, X_test_vec, y_test_vec):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Run SVM - fit and predict\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     SVM             \u001b[38;5;241m\u003d\u001b[39m OneVsRestClassifier(LinearSVC(dual\u001b[38;5;241m\u003d\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, class_weight\u001b[38;5;241m\u003d\u001b[39m\u001b[38;5;124m\u0027\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\u0027\u001b[39m), n_jobs\u001b[38;5;241m\u003d\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----\u003e 5\u001b[0m     \u001b[43mSVM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m\u003d\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     prediction      \u001b[38;5;241m\u003d\u001b[39m SVM\u001b[38;5;241m.\u001b[39mpredict(X_test_vec)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Evaluate the results\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument \u0027n_classes\u0027"
          ]
        }
      ],
      "source": "print(\"-------------- No noisy training sets ----------\")\nfor size in [2000, 4000, 5000, 8000, 10000, 15000, 20000]:\n    # Get a training set without noisy data\n    X_train, y_train \u003d dc.get_train(size, train_distribution)\n    print(\"* Training set size: %d samples: \" % (len(X_train)))\n    print_distribution(\"  Sentiment distribution\", y_train)\n\n    # Do an experiment\n    do_experiment(X_train, y_train, X_test, y_test)\n\nprint(\"-------------- Noisy training sets -------------\")\nprint(\"The proportions of the noise sources %s: \" % [x[0] for x in lstNoisyInfo],\n      [round(x[1]*100/dc.get_noisy_len(),1) for x in lstNoisyInfo])\nfor size in [(4000, 1000), (8000, 3000), (15000, 5000)]:\n    # Get a noisy training set\n    X_train, y_train \u003d dc.get_train_with_noisy(size[0], size[1], train_distribution)\n    print(\"* Noisy training set size: %d samples (%d original, %d noisy)\" % (len(y_train), size[0], size[1]))\n    print_distribution(\"  Sentiment distribution\", y_train)\n\n    # Do an experiment\n    do_experiment(X_train, y_train, X_test, y_test)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88e38ecd",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "stem_cell": {
      "cell_type": "raw",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}