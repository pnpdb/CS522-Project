{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac875a5d",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# SVM - Climate Sentiment Multiclass Classification\n",
    "## CS522 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c04439",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Dataset: \n",
    "https://www.kaggle.com/code/luiskalckstein/climate-sentiment-multiclass-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9502e",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a471b8ce",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 155 µs (started: 2022-04-09 10:40:09 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from Common.DataCenter import data_center\n",
    "from Common.LSI import SKLearnLSA\n",
    "from Common.UtilFuncs import DataSize\n",
    "%matplotlib inline\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime\n",
    "    \n",
    "TrainSizeBaseLine = DataSize.GetTrainSizeBaseline()\n",
    "TrainSizeWithNoisyData = DataSize.GetTrainSizeWithNoisyData()\n",
    "TestDataSize = DataSize.GetTestDataSize()\n",
    "NoiseDataSize = DataSize.GetNoiseDataSize()\n",
    "ValidationDataSize = DataSize.GetValidationDataSize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65bb9a6",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b97cc85",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 238 µs (started: 2022-04-09 10:40:09 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# parameter: original X of training set and test set\n",
    "# return:  vectorised X of training set and test set\n",
    "def text_preprocessing(X_train, X_test):\n",
    "    # Convert texts to vectors\n",
    "    lsa = SKLearnLSA()\n",
    "    lsa.BuildModel(X_train + X_test, 800)\n",
    "    X_train_vec = lsa.Query2LatentSpace(X_train)\n",
    "    X_test_vec = lsa.Query2LatentSpace(X_test)\n",
    "    return X_train_vec, X_test_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a754f",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### One-hot encoding, convert the labels to vectors (4 x 1) each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167ffdc7",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 206 µs (started: 2022-04-09 10:40:09 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# parameter: original y of training set, original y of test set\n",
    "# return:  encoded y of training set and test set\n",
    "def one_hot_encoding(y_train, y_test):\n",
    "    mlb          = MultiLabelBinarizer()\n",
    "    y_train_vec  = mlb.fit_transform(map(str, y_train))\n",
    "    y_test_vec   = mlb.transform(map(str, y_test))\n",
    "    return y_train_vec, y_test_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16eefc7",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Run SVM and evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc133f80",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 374 µs (started: 2022-04-09 10:40:09 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# parameter:  vectorised X and encoded y of training set and test set\n",
    "def evaluate_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec):\n",
    "    # Run SVM - fit and predict\n",
    "    #SVM             = OneVsRestClassifier(LinearSVC(dual=False, class_weight=\"balanced\"), n_jobs=-1)\n",
    "    SVM = OneVsRestClassifier(SVC(gamma='auto', class_weight=\"balanced\"), n_jobs=-1)\n",
    "    SVM.fit(X_train_vec, y_train_vec)\n",
    "    prediction      = SVM.predict(X_test_vec)\n",
    "\n",
    "    # Evaluate the results\n",
    "    macro_f1        = f1_score(y_test_vec, prediction, average='macro')\n",
    "    weighted_f1     = f1_score(y_test_vec, prediction, average='weighted')\n",
    "    macro_precision = precision_score(y_test_vec, prediction, average='macro')\n",
    "    macro_recall    = recall_score(y_test_vec, prediction, average='macro')\n",
    "\n",
    "    return macro_f1, weighted_f1, macro_precision, macro_recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e3459",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Do an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52dbb13f",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 317 µs (started: 2022-04-09 10:40:09 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Parameter: original X,y of training set and test set\n",
    "def do_experiment(X_train, y_train, X_test, y_test):\n",
    "    # Convert texts to vectors\n",
    "    X_train_vec, X_test_vec = text_preprocessing(X_train, X_test)\n",
    "    y_train_vec, y_test_vec = one_hot_encoding(y_train, y_test)\n",
    "\n",
    "    # Run SVM and evaluate the results\n",
    "    macro_f1, weighted_f1, macro_precision, macro_recall = \\\n",
    "        evaluate_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec)\n",
    "\n",
    "    # Show the indicators\n",
    "    print(\" macro_f1: %.4f , weighted_f1: %.4f, macro_precision: %.4f, macro_recall: %.4f\" %\n",
    "          (macro_f1, weighted_f1, macro_precision, macro_recall))\n",
    "    return X_train_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a7cfb",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Main entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381feb39",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Load the database and split it into training set, test set, noisy set, validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd482f7a",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################\n",
      "Total data size:  40908\n",
      "Total train data size:  32908\n",
      "Total test data size:  4000\n",
      "time: 81.8 ms (started: 2022-04-09 10:40:09 +08:00)\n"
     ]
    }
   ],
   "source": [
    "dc = data_center(\"twitter_sentiment_data_clean.csv\", test_size=TestDataSize, noisy_size=NoiseDataSize, validation_size=ValidationDataSize)\n",
    "\n",
    "print(\"####################################################\")\n",
    "print(\"Total data size: \",       dc.get_len())\n",
    "print(\"Total train data size: \", dc.get_train_len())\n",
    "print(\"Total test data size: \",  dc.get_test_len())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63246fd4",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Get the test set for evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a13e38-2ff1-46dd-8ebf-e8c7a02a4326",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 141 µs (started: 2022-04-09 10:40:09 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# distribution of training set\n",
    "train_distribution = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb695e-8222-49f1-baa1-7c97c3504324",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Prepare the noisy set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915f37b2-3217-44af-b258-caa0fb20a704",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy set size is 3000\n",
      "./Noisy/irrelevant/Twitter and Reddit Sentimental analysis Dataset.csv\n",
      "./Noisy/irrelevant/Liberals vs Conservatives on Reddit.csv\n",
      "./Noisy/irrelevant/Twitter Sentiment Analysis.csv\n",
      "./Noisy/irrelevant/Portuguese Tweets for Sentiment Analysis(外文).csv\n",
      "./Noisy/irrelevant/Tweets with Sarcasm and Irony.csv\n",
      "./Noisy/irrelevant/Disaster Tweets.csv\n",
      "./Noisy/irrelevant/Odia News Dataset(foreign).csv\n",
      "./Noisy/irrelevant/CNN_Articels_clean.csv\n",
      "./Noisy/irrelevant/IMDB Movie Ratings Sentiment Analysis.csv\n",
      "./Noisy/irrelevant/fake_new.csv\n",
      "./Noisy/irrelevant/Amazon Product Reviews.csv\n",
      "./Noisy/irrelevant/covid19-tweets.csv\n",
      "Cannot generate the set with size 3000 and distribution None!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoisy set size is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m                \u001b[38;5;241m%\u001b[39m dc\u001b[38;5;241m.\u001b[39mget_noisy_len())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# add the external noisy data (irrelevant texts)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m added_size \u001b[38;5;241m=\u001b[39m \u001b[43mdc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_noisy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mirrelevant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexternal_noisy_distribution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6000\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# max size: 34259\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m noisy samples added\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m added_size)\n\u001b[1;32m     10\u001b[0m lstNoisyInfo\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mirrelevant\u001b[39m\u001b[38;5;124m\"\u001b[39m,added_size))\n",
      "File \u001b[0;32m~/Documents/workspace/CS522-Project/Common/DataCenter.py:211\u001b[0m, in \u001b[0;36mdata_center.add_noisy\u001b[0;34m(self, noisy_source, distribution, size)\u001b[0m\n\u001b[1;32m    208\u001b[0m     df  \u001b[38;5;241m=\u001b[39m df[:size]\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfNoisy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfNoisy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_original_noisy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfNoisy    \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfNoisy, df])\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df)\n",
      "File \u001b[0;32m~/Documents/workspace/CS522-Project/Common/DataCenter.py:114\u001b[0m, in \u001b[0;36mdata_center.get_original_noisy\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoisy_size:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe size is large than the max original noisy size!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoisy_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Change labels to make noise\u001b[39;00m\n\u001b[1;32m    117\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrseed)\n",
      "File \u001b[0;32m~/Documents/workspace/CS522-Project/Common/DataCenter.py:63\u001b[0m, in \u001b[0;36mdata_center.__get_subset\u001b[0;34m(self, start, set_size, size, distribution)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(set_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution[i])\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot generate the set with size \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m and distribution \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     62\u001b[0m           \u001b[38;5;241m%\u001b[39m (size, \u001b[38;5;28mstr\u001b[39m(distribution)))\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     65\u001b[0m     df  \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfs[i][s : s \u001b[38;5;241m+\u001b[39m c]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 545 ms (started: 2022-04-09 10:40:09 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# distribution of external noisy\n",
    "external_noisy_distribution = [0.25, 0.25, 0.25, 0.25]\n",
    "lstNoisyInfo = [(\"mislabeled\",dc.get_noisy_len())]\n",
    "print(\"Noisy set size is %d\"                % dc.get_noisy_len())\n",
    "\n",
    "# add the external noisy data (irrelevant texts)\n",
    "added_size = dc.add_noisy(noisy_source=\"irrelevant\",\n",
    "                          distribution = external_noisy_distribution, size = 6000) # max size: 34259\n",
    "print(\"%d noisy samples added\" % added_size)\n",
    "lstNoisyInfo.append((\"irrelevant\",added_size))\n",
    "\n",
    "# add the external noisy data (translated texts)\n",
    "added_size = dc.add_noisy(noisy_source=\"translated\",\n",
    "                          distribution = \"reserve_labels\", size = 6000) # max size: 6146\n",
    "print(\"%d noisy samples added\" % added_size)\n",
    "lstNoisyInfo.append((\"translated\",added_size))\n",
    "\n",
    "print(\"Noisy set new size is %d\"                % dc.get_noisy_len())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c00bd3",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test = dc.get_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dab526",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Run experiments with different training sets, and use the same test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b800dd",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"-----------------------------------------------\")\n",
    "for size in TrainSizeBaseLine:\n",
    "    # Get a training set without noisy data\n",
    "    X_train, y_train = dc.get_train(size, train_distribution)\n",
    "    print(\"Training set size: %d samples (%.1f%%): \" % (len(X_train), len(y_train)/dc.get_train_len()*100))\n",
    "\n",
    "    # Do an experiment\n",
    "    do_experiment(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"-----------------------------------------------\")\n",
    "xtrainvec = None\n",
    "for size in TrainSizeWithNoisyData:\n",
    "    # Get a noisy training set\n",
    "    X_train, y_train = dc.get_train_with_noisy(size[0], size[1], train_distribution)\n",
    "    print(\"Noisy training set size: %d samples (%d original, %d noisy)\" % (len(y_train), size[0], size[1]))\n",
    "\n",
    "    # Do an experiment\n",
    "    xtrainvec = do_experiment(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b5bb1",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = dc.get_train(2000)\n",
    "# Convert texts to vectors\n",
    "X_train_vec, X_test_vec = text_preprocessing(X_train, X_test)\n",
    "y_train_vec, y_test_vec = one_hot_encoding(y_train, y_test)\n",
    "\n",
    "# Run SVM and evaluate the results\n",
    "macro_f1, weighted_f1, macro_precision, macro_recall = \\\n",
    "    evaluate_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec)\n",
    "\n",
    "# Show the indicators\n",
    "print(\" macro_f1: %.4f , weighted_f1: %.4f, macro_precision: %.4f, macro_recall: %.4f\" %\n",
    "        (macro_f1, weighted_f1, macro_precision, macro_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c9560-6aba-4740-b4aa-158f724fa89b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
