{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac875a5d",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# SVM - Climate Sentiment Multiclass Classification\n",
    "## CS522 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48014012-7e18-4fb1-8f8d-b0ed2f49008f",
   "metadata": {},
   "source": [
    "SVM with LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c04439",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Dataset: \n",
    "https://www.kaggle.com/code/luiskalckstein/climate-sentiment-multiclass-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9502e",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a471b8ce",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 154 µs (started: 2022-04-10 00:17:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from Common.DataCenter import data_center\n",
    "from Common.LSI import SKLearnLSA\n",
    "from Common.UtilFuncs import DataSize\n",
    "from Common.UtilFuncs import print_evaluation, EvaluationToDF\n",
    "import pandas as pd\n",
    "from Common.preprocessor import normalize_preprocessing\n",
    "%matplotlib inline\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime\n",
    "    \n",
    "TrainSizeBaseLine = DataSize.GetTrainSizeBaseline()\n",
    "TrainSizeWithNoisyData = DataSize.GetTrainSizeWithNoisyData()\n",
    "TestDataSize = DataSize.GetTestDataSize()\n",
    "NoiseDataSize = DataSize.GetNoiseDataSize()\n",
    "ValidationDataSize = DataSize.GetValidationDataSize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65bb9a6",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b97cc85",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 274 µs (started: 2022-04-10 00:17:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# parameter: original X of training set and test set\n",
    "# return:  vectorised X of training set and test set\n",
    "def text_preprocessing(X_train, X_test):\n",
    "    # Convert texts to vectors\n",
    "    X_train = normalize_preprocessing(X_train)\n",
    "    X_test = normalize_preprocessing(X_test)\n",
    "    lsa = SKLearnLSA()\n",
    "    lsa.BuildModel(X_train + X_test, 2000)\n",
    "    X_train_vec = lsa.Query2LatentSpace(X_train)\n",
    "    X_test_vec = lsa.Query2LatentSpace(X_test)\n",
    "    return X_train_vec, X_test_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a754f",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### One-hot encoding, convert the labels to vectors (4 x 1) each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167ffdc7",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 215 µs (started: 2022-04-10 00:17:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# parameter: original y of training set, original y of test set\n",
    "# return:  encoded y of training set and test set\n",
    "def one_hot_encoding(y_train, y_test):\n",
    "    mlb          = MultiLabelBinarizer()\n",
    "    y_train_vec  = mlb.fit_transform(map(str, y_train))\n",
    "    y_test_vec   = mlb.transform(map(str, y_test))\n",
    "    return y_train_vec, y_test_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16eefc7",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Run SVM and evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc133f80",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 297 µs (started: 2022-04-10 00:17:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# parameter:  vectorised X and encoded y of training set and test set\n",
    "def evaluate_SVM(title, X_train_vec, y_train_vec, X_test_vec, y_test_vec):\n",
    "    # Run SVM - fit and predict\n",
    "    SVM             = OneVsRestClassifier(LinearSVC(dual=False, class_weight=\"balanced\"), n_jobs=-1)\n",
    "    #SVM = OneVsRestClassifier(SVC(gamma='auto', class_weight=\"balanced\"), n_jobs=-1)\n",
    "    SVM.fit(X_train_vec, y_train_vec)\n",
    "    prediction      = SVM.predict(X_test_vec)\n",
    "    print_evaluation(y_test_vec, prediction)\n",
    "    evaluateDF = EvaluationToDF(title, y_test_vec, prediction)\n",
    "\n",
    "    return evaluateDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e3459",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Do an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52dbb13f",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 321 µs (started: 2022-04-10 00:17:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Parameter: original X,y of training set and test set\n",
    "def do_experiment(title, X_train, y_train, X_test, y_test):\n",
    "    # Convert texts to vectors\n",
    "    X_train_vec, X_test_vec = text_preprocessing(X_train, X_test)\n",
    "    y_train_vec, y_test_vec = one_hot_encoding(y_train, y_test)\n",
    "\n",
    "    # Run SVM and evaluate the results\n",
    "    evaluateDF = \\\n",
    "        evaluate_SVM(title, X_train_vec, y_train_vec, X_test_vec, y_test_vec)\n",
    "\n",
    "    # Show the indicators\n",
    "    #print(\" macro_f1: %.4f , weighted_f1: %.4f, macro_precision: %.4f, macro_recall: %.4f\" %\n",
    "    #      (macro_f1, weighted_f1, macro_precision, macro_recall))\n",
    "    #print(evaluateDF)\n",
    "    return evaluateDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a7cfb",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Main entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358c10bb-6714-4a46-b044-77bd41026edf",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################\n",
      "Total data size:  40908\n",
      "Total train data size:  30908\n",
      "Total test data size:  4000\n",
      "time: 131 ms (started: 2022-04-10 00:17:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noisy_set_sizes = {\n",
    "    'mislabeled' : 5000,   # max size: 15000\n",
    "    'irrelevant' : 5000,   # max size: 34259\n",
    "    'translated' : 5000,   # max size: 5000\n",
    "}\n",
    "\n",
    "# Load the database and split it into training set, test set, noisy set, validation set\n",
    "dc = data_center(\"twitter_sentiment_data_clean.csv\", test_size = 4000, validation_size = 1000,\n",
    "                 noisy_size = noisy_set_sizes['mislabeled'])\n",
    "\n",
    "print(\"####################################################\")\n",
    "print(\"Total data size: \",       dc.get_len())\n",
    "print(\"Total train data size: \", dc.get_train_len())\n",
    "print(\"Total test data size: \",  dc.get_test_len())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a6683-66ba-4222-891a-b102a0e98bba",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Get the test set for evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1892e956-46ef-4fe9-a8b0-fe9350bb5711",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.29 ms (started: 2022-04-10 00:17:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = dc.get_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513f812-db00-4b43-bfd9-a8517cfc0815",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Set distributions of training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0c70b33-1999-465b-8839-e0d1835ebf1e",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 152 µs (started: 2022-04-10 00:17:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# distribution of training set\n",
    "train_distribution = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d832d1-c036-4cf7-9d33-691d04681f08",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Prepare the noisy set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d661285-4571-4d92-a0e0-80ccc4145896",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy set size is 5000\n",
      "5000 noisy samples added\n",
      "5000 noisy samples added\n",
      "Noisy set new size is 15000\n",
      "time: 254 ms (started: 2022-04-10 00:17:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "lstNoisyInfo = [(\"mislabeled\",dc.get_noisy_len())]\n",
    "print(\"Noisy set size is %d\"                % dc.get_noisy_len())\n",
    "\n",
    "# add the external noisy data (irrelevant texts)\n",
    "# distribution of irrelevant noisy\n",
    "irrelevant_noisy_distribution = [0.25, 0.25, 0.25, 0.25]    # None, if use the distribution of original set\n",
    "added_size = dc.add_noisy(noisy_source=\"irrelevant\", distribution = irrelevant_noisy_distribution,\n",
    "                          size = noisy_set_sizes['irrelevant'])\n",
    "print(\"%d noisy samples added\" % added_size)\n",
    "lstNoisyInfo.append((\"irrelevant\",added_size))\n",
    "\n",
    "# add the external noisy data (translated texts). use the labels of each noisy data\n",
    "added_size = dc.add_noisy(noisy_source=\"translated\", distribution = \"reserve_labels\", \n",
    "                          size = noisy_set_sizes['translated'])\n",
    "print(\"%d noisy samples added\" % added_size)\n",
    "lstNoisyInfo.append((\"translated\",added_size))\n",
    "\n",
    "print(\"Noisy set new size is %d\"                % dc.get_noisy_len())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381feb39",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Load the database and split it into training set, test set, noisy set, validation set**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63246fd4",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Get the test set for evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dab526",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Run experiments with different training sets, and use the same test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58b800dd",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Training set size: 2000 samples (6.5%): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  f1 of classes: [0.339, 0.396, 0.639, 0.563]\n",
      "  micro_f1: 0.541 , macro_f1: 0.484 , weighted_f1: 0.550, macro_precision: 0.460, macro_recall: 0.517\n",
      "Training set size: 4000 samples (12.9%): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  f1 of classes: [0.353, 0.397, 0.659, 0.591]\n",
      "  micro_f1: 0.560 , macro_f1: 0.500 , weighted_f1: 0.568, macro_precision: 0.470, macro_recall: 0.540\n",
      "Training set size: 5000 samples (16.2%): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  f1 of classes: [0.374, 0.422, 0.662, 0.588]\n",
      "  micro_f1: 0.567 , macro_f1: 0.512 , weighted_f1: 0.575, macro_precision: 0.476, macro_recall: 0.558\n",
      "Training set size: 8000 samples (25.9%): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  f1 of classes: [0.382, 0.464, 0.695, 0.605]\n",
      "  micro_f1: 0.592 , macro_f1: 0.537 , weighted_f1: 0.603, macro_precision: 0.488, macro_recall: 0.607\n",
      "Training set size: 10000 samples (32.4%): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  f1 of classes: [0.372, 0.464, 0.704, 0.632]\n",
      "  micro_f1: 0.599 , macro_f1: 0.543 , weighted_f1: 0.613, macro_precision: 0.491, macro_recall: 0.623\n",
      "Training set size: 15000 samples (48.5%): \n",
      "  f1 of classes: [0.385, 0.492, 0.731, 0.669]\n",
      "  micro_f1: 0.625 , macro_f1: 0.569 , weighted_f1: 0.641, macro_precision: 0.507, macro_recall: 0.670\n",
      "Training set size: 20000 samples (64.7%): \n",
      "  f1 of classes: [0.398, 0.507, 0.743, 0.675]\n",
      "  micro_f1: 0.635 , macro_f1: 0.581 , weighted_f1: 0.653, macro_precision: 0.512, macro_recall: 0.698\n",
      "-----------------------------------------------\n",
      "Noisy training set size: 5000 samples (4000 original, 1000 noisy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  f1 of classes: [0.35, 0.435, 0.703, 0.643]\n",
      "  micro_f1: 0.592 , macro_f1: 0.533 , weighted_f1: 0.607, macro_precision: 0.466, macro_recall: 0.644\n",
      "Noisy training set size: 10000 samples (8000 original, 2000 noisy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  f1 of classes: [0.378, 0.463, 0.717, 0.647]\n",
      "  micro_f1: 0.602 , macro_f1: 0.551 , weighted_f1: 0.623, macro_precision: 0.471, macro_recall: 0.704\n",
      "Noisy training set size: 20000 samples (15000 original, 5000 noisy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  f1 of classes: [0.396, 0.484, 0.733, 0.644]\n",
      "  micro_f1: 0.613 , macro_f1: 0.564 , weighted_f1: 0.636, macro_precision: 0.476, macro_recall: 0.739\n",
      "time: 1h 14min 5s (started: 2022-04-10 00:17:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "evaluateDF = None\n",
    "print(\"-----------------------------------------------\")\n",
    "for size in TrainSizeBaseLine:\n",
    "    # Get a training set without noisy data\n",
    "    X_train, y_train = dc.get_train(size, train_distribution)\n",
    "    print(\"Training set size: %d samples (%.1f%%): \" % (len(X_train), len(y_train)/dc.get_train_len()*100))\n",
    "\n",
    "    # Do an experiment\n",
    "    title = \"%d\" % (len(X_train))\n",
    "    df = do_experiment(title, X_train, y_train, X_test, y_test)\n",
    "    if evaluateDF is None:\n",
    "        evaluateDF = df\n",
    "    else:\n",
    "        evaluateDF = pd.concat([evaluateDF,df],axis=0)\n",
    "\n",
    "print(\"-----------------------------------------------\")\n",
    "xtrainvec = None\n",
    "for size in TrainSizeWithNoisyData:\n",
    "    # Get a noisy training set\n",
    "    X_train, y_train = dc.get_train_with_noisy(size[0], size[1], train_distribution)\n",
    "    print(\"Noisy training set size: %d samples (%d original, %d noisy)\" % (len(y_train), size[0], size[1]))\n",
    "\n",
    "    # Do an experiment\n",
    "    title = \"%d samples (%d original, %d noisy)\" % (len(y_train), size[0], size[1])\n",
    "    df = do_experiment(title, X_train, y_train, X_test, y_test)\n",
    "    if evaluateDF is None:\n",
    "        evaluateDF = df\n",
    "    else:\n",
    "        evaluateDF = pd.concat([evaluateDF,df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b12b5bb1",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.2 ms (started: 2022-04-10 01:31:32 +08:00)\n"
     ]
    }
   ],
   "source": [
    "evaluateDF.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c9560-6aba-4740-b4aa-158f724fa89b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
