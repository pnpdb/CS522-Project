{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8b69a9",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# SVM - Climate Sentiment Multiclass Classification\n",
    "\n",
    "## CS522 Project, Robust Covariance Denoise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37f738",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Dataset: \n",
    "https://www.kaggle.com/code/luiskalckstein/climate-sentiment-multiclass-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac19b9",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9e24e3",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 96.4 µs (started: 2022-04-17 10:55:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from Common.DataCenter import data_center\n",
    "from Common.preprocessor import normalize_preprocessing\n",
    "from Common.UtilFuncs import print_evaluation, print_distribution\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4394c469-884b-4401-a951-bc16b9be3cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 55.9 ms (started: 2022-04-17 10:55:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.array([[0,1], [1,1], [1,2], [2,2], [5,6]]), columns = [\"x\", \"y\"], index = [0,1,2,3,4])\n",
    "# show data in a scatterplot\n",
    "plt.scatter(df[\"x\"], df[\"y\"], color = \"r\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001be003-047b-4280-98af-054109cd6f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EllipticEnvelope()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.9 ms (started: 2022-04-17 10:55:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# convert dataframe to arrays\n",
    "data = df[['x', 'y']].values\n",
    "# instantiate model\n",
    "model1 = EllipticEnvelope(contamination = 0.1) \n",
    "# fit model\n",
    "model1.fit(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3b5576-ea5f-4367-8d0a-baec46f25335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1 -1]\n",
      "time: 875 µs (started: 2022-04-17 10:55:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "pred1 = model1.predict(data)\n",
    "print(pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c56d669",
   "metadata": {
    "pycharm": {
     "metadata": false
    },
    "tags": []
   },
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb566c07",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 424 µs (started: 2022-04-17 10:55:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# parameter: original X of training set and test set\n",
    "# return:  vectorised X of training set and test set\n",
    "def text_preprocessing(X_train, X_test):\n",
    "    \n",
    "    # preprocessing with traditional NLP methodology\n",
    "    X_train_normalized = normalize_preprocessing(X_train)\n",
    "    X_test_normalized  = normalize_preprocessing(X_test)\n",
    "    \n",
    "    # vectorization\n",
    "    # Convert texts to vectors by TFIDF\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "    X_train_vec  = vectorizer.fit_transform(X_train_normalized)\n",
    "    X_test_vec   = vectorizer.transform(X_test_normalized)\n",
    "      \n",
    "    return X_train_vec, X_test_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08bb41b",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### One-hot encoding, convert the labels to vectors (4 x 1) each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae54a70",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 377 µs (started: 2022-04-17 10:55:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# parameter: original y of training set, original y of test set\n",
    "# return:  encoded y of training set and test set\n",
    "def one_hot_encoding(y_train, y_test):\n",
    "    mlb          = MultiLabelBinarizer()\n",
    "    y_train_vec  = mlb.fit_transform(map(str, y_train))\n",
    "    y_test_vec   = mlb.transform(map(str, y_test))\n",
    "    return y_train_vec, y_test_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46673138",
   "metadata": {
    "pycharm": {
     "metadata": false
    },
    "tags": []
   },
   "source": [
    "### Run SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fb5965e",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 366 µs (started: 2022-04-17 10:55:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# parameter:  vectorised X and encoded y of training set and test set\n",
    "def run_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec):\n",
    "    # Run SVM - fit and predict\n",
    "    SVM             = OneVsRestClassifier(LinearSVC(dual=False, class_weight='balanced'), n_jobs=-1)\n",
    "    SVM.fit(X_train_vec, y_train_vec)\n",
    "    y_pred          = SVM.predict(X_test_vec)\n",
    "    return  y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee21ce",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Do an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23ad12b9",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 777 µs (started: 2022-04-17 10:55:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Parameter: original X,y of training set and test set\n",
    "def do_experiment(X_train, y_train, X_test, y_test):\n",
    "    # Convert texts to vectors\n",
    "    X_train_vec, X_test_vec = text_preprocessing(X_train, X_test)\n",
    "    y_train_vec, y_test_vec = one_hot_encoding(y_train, y_test)\n",
    "\n",
    "    # Run SVM and evaluate the results\n",
    "    y_pred = run_SVM(X_train_vec, y_train_vec, X_test_vec, y_test_vec)\n",
    "\n",
    "    # Print the evaluation\n",
    "    print_evaluation(y_test_vec, y_pred, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e4885",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Main entry\n",
    "**Load the database and split it into training set, test set, noisy set, validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a40f2",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a95718",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# The size of the noise sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd1e13d",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################\n",
      "Total data size:  40908\n",
      "Total train data size:  20000\n",
      "Total test data size:  4000\n",
      "time: 138 ms (started: 2022-04-17 10:55:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noisy_set_sizes = {\n",
    "    'mislabeled' : 5000,   # max size: 15000\n",
    "    'irrelevant' : 5000,   # max size: 34259\n",
    "    'translated' : 5000,   # max size: 5000\n",
    "}\n",
    "\n",
    "# Load the database and split it into training set, test set, noisy set, validation set\n",
    "dc = data_center(\"twitter_sentiment_data_clean.csv\", train_size = 20000, test_size = 4000, validation_size = 1000,\n",
    "                 noisy_size = noisy_set_sizes['mislabeled'] if 'mislabeled' in noisy_set_sizes.keys() else 0)\n",
    "\n",
    "print(\"####################################################\")\n",
    "print(\"Total data size: \",       dc.get_len())\n",
    "print(\"Total train data size: \", dc.get_train_len())\n",
    "print(\"Total test data size: \",  dc.get_test_len())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034dbef",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Get the test set for evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ced5f51",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.54 ms (started: 2022-04-17 10:55:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = dc.get_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf572713",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Set distributions of training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb09a89",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 235 µs (started: 2022-04-17 10:55:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# distribution of training set\n",
    "train_distribution = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29163adf",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Prepare the noisy set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972eda65",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 noisy samples of 'mislabeled' added\n",
      "5000 noisy samples of 'irrelevant' added\n",
      "5000 noisy samples of 'translated' added\n",
      "The total size of noisy data is 15000\n",
      "time: 363 ms (started: 2022-04-17 10:55:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "lstNoisyInfo    = []\n",
    "if 'mislabeled' in noisy_set_sizes.keys() and noisy_set_sizes['mislabeled'] > 0:\n",
    "    lstNoisyInfo.append((\"mislabeled\",dc.get_noisy_len()))\n",
    "    print(\"%d noisy samples of '%s' added\" % (dc.get_noisy_len(), 'mislabeled'))\n",
    "\n",
    "# add the external noisy data (irrelevant texts)\n",
    "if 'irrelevant' in noisy_set_sizes.keys() and noisy_set_sizes['irrelevant'] > 0:\n",
    "    # distribution of irrelevant noisy\n",
    "    irrelevant_noisy_distribution = [0.25, 0.25, 0.25, 0.25]    # None, if use the distribution of original set\n",
    "    added_size = dc.add_noisy(noisy_source=\"irrelevant\", distribution = irrelevant_noisy_distribution,\n",
    "                              size = noisy_set_sizes['irrelevant'])\n",
    "    print(\"%d noisy samples of '%s' added\"  % (added_size, 'irrelevant'))\n",
    "    lstNoisyInfo.append((\"irrelevant\",added_size))\n",
    "\n",
    "# add the external noisy data (translated texts). use the labels of each noisy data\n",
    "if 'translated' in noisy_set_sizes.keys() and noisy_set_sizes['translated'] > 0:\n",
    "    added_size = dc.add_noisy(noisy_source=\"translated\", distribution = \"reserve_labels\",\n",
    "                              size = noisy_set_sizes['translated'])\n",
    "    print(\"%d noisy samples of '%s' added\"  % (added_size, 'translated'))\n",
    "    lstNoisyInfo.append((\"translated\",added_size))\n",
    "\n",
    "print(\"The total size of noisy data is %d\"                % dc.get_noisy_len())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dccb7f5",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "**Run experiments with different training sets, and use the same test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc9be9ab",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- No noisy training sets ----------\n",
      "* Training set size: 2000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.349, 0.315, 0.711, 0.678]\n",
      "  micro_f1: 0.626 , macro_f1: 0.513 , weighted_f1: 0.597, macro_precision: 0.592, macro_recall: 0.504\n",
      "* Training set size: 4000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.442, 0.371, 0.741, 0.695]\n",
      "  micro_f1: 0.657 , macro_f1: 0.562 , weighted_f1: 0.635, macro_precision: 0.623, macro_recall: 0.554\n",
      "* Training set size: 5000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.466, 0.4, 0.746, 0.709]\n",
      "  micro_f1: 0.668 , macro_f1: 0.580 , weighted_f1: 0.648, macro_precision: 0.636, macro_recall: 0.570\n",
      "* Training set size: 8000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.521, 0.42, 0.758, 0.719]\n",
      "  micro_f1: 0.682 , macro_f1: 0.604 , weighted_f1: 0.665, macro_precision: 0.648, macro_recall: 0.598\n",
      "* Training set size: 10000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.534, 0.431, 0.765, 0.727]\n",
      "  micro_f1: 0.689 , macro_f1: 0.614 , weighted_f1: 0.674, macro_precision: 0.645, macro_recall: 0.613\n",
      "* Training set size: 15000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.567, 0.452, 0.774, 0.74]\n",
      "  micro_f1: 0.702 , macro_f1: 0.633 , weighted_f1: 0.688, macro_precision: 0.657, macro_recall: 0.637\n",
      "* Training set size: 20000 samples: \n",
      "  Sentiment distribution: 9.4%, 18.3%, 50.2%, 22.1%\n",
      "  f1 of classes: [0.599, 0.483, 0.788, 0.746]\n",
      "  micro_f1: 0.716 , macro_f1: 0.654 , weighted_f1: 0.705, macro_precision: 0.668, macro_recall: 0.660\n",
      "-------------- Noisy training sets -------------\n",
      "The proportions of the noise sources ['mislabeled', 'irrelevant', 'translated']:  [33.3, 33.3, 33.3]\n",
      "* Noisy training set size: 5000 samples (4000 original, 1000 noisy)\n",
      "  Sentiment distribution: 11.8%, 19.8%, 46.1%, 22.3%\n",
      "  f1 of classes: [0.371, 0.344, 0.731, 0.678]\n",
      "  micro_f1: 0.639 , macro_f1: 0.531 , weighted_f1: 0.615, macro_precision: 0.595, macro_recall: 0.521\n",
      "* Noisy training set size: 10000 samples (8000 original, 2000 noisy)\n",
      "  Sentiment distribution: 12.0%, 19.7%, 45.8%, 22.5%\n",
      "  f1 of classes: [0.472, 0.411, 0.746, 0.706]\n",
      "  micro_f1: 0.667 , macro_f1: 0.584 , weighted_f1: 0.650, macro_precision: 0.632, macro_recall: 0.572\n",
      "* Noisy training set size: 20000 samples (15000 original, 5000 noisy)\n",
      "  Sentiment distribution: 13.1%, 19.6%, 44.2%, 23.0%\n",
      "  f1 of classes: [0.519, 0.452, 0.762, 0.728]\n",
      "  micro_f1: 0.687 , macro_f1: 0.615 , weighted_f1: 0.675, macro_precision: 0.642, macro_recall: 0.611\n",
      "time: 40.8 s (started: 2022-04-17 10:55:36 +08:00)\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------- No noisy training sets ----------\")\n",
    "for size in [2000, 4000, 5000, 8000, 10000, 15000, 20000]:\n",
    "    # Get a training set without noisy data\n",
    "    X_train, y_train = dc.get_train(size, train_distribution)\n",
    "    print(\"* Training set size: %d samples: \" % (len(X_train)))\n",
    "    print_distribution(\"  Sentiment distribution\", y_train)\n",
    "\n",
    "    # Do an experiment\n",
    "    do_experiment(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"-------------- Noisy training sets -------------\")\n",
    "print(\"The proportions of the noise sources %s: \" % [x[0] for x in lstNoisyInfo],\n",
    "      [round(x[1]*100/dc.get_noisy_len(),1) for x in lstNoisyInfo])\n",
    "for size in [(4000, 1000), (8000, 2000), (15000, 5000)]:\n",
    "    # Get a noisy training set\n",
    "    X_train, y_train = dc.get_train_with_noisy(size[0], size[1], train_distribution)\n",
    "    print(\"* Noisy training set size: %d samples (%d original, %d noisy)\" % (len(y_train), size[0], size[1]))\n",
    "    print_distribution(\"  Sentiment distribution\", y_train)\n",
    "\n",
    "    # Do an experiment\n",
    "    do_experiment(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d798d38",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor size in [(4000, 1000), (8000, 2000), (15000, 5000)]:\\n    # Here we have the noisy data set\\n    X_train, y_train = dc.get_train_with_noisy(size[0], size[1], train_distribution)\\n    print(\"* Noisy training set size: %d samples (%d original, %d noisy)\" % (len(y_train), size[0], size[1]))\\n    print_distribution(\"  Sentiment distribution\", y_train)\\n    # Do an experiment\\n    do_experiment(X_train, y_train, X_test, y_test)\\n    \\n    svd = SVD(4000)\\n    # now I try to remove the noise\\n    # instantiate model\\n    model2 = EllipticEnvelope(contamination = 0.1) \\n    # fit model\\n    result = model2.fit_predict(svd.U)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.6 ms (started: 2022-04-17 10:56:16 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from Common.SVDUtils import SVD\n",
    "'''\n",
    "for size in [(4000, 1000), (8000, 2000), (15000, 5000)]:\n",
    "    # Here we have the noisy data set\n",
    "    X_train, y_train = dc.get_train_with_noisy(size[0], size[1], train_distribution)\n",
    "    print(\"* Noisy training set size: %d samples (%d original, %d noisy)\" % (len(y_train), size[0], size[1]))\n",
    "    print_distribution(\"  Sentiment distribution\", y_train)\n",
    "    # Do an experiment\n",
    "    do_experiment(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    svd = SVD(4000)\n",
    "    # now I try to remove the noise\n",
    "    # instantiate model\n",
    "    model2 = EllipticEnvelope(contamination = 0.1) \n",
    "    # fit model\n",
    "    result = model2.fit_predict(svd.U)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68846307-0c0d-4175-814c-51ff313acd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 336 µs (started: 2022-04-17 10:56:16 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# parameter: original X of training set and test set\n",
    "# return:  vectorised X of training set and test set\n",
    "def text_preprocessing_fordata(X_train):\n",
    "    \n",
    "    # preprocessing with traditional NLP methodology\n",
    "    X_train_normalized = normalize_preprocessing(X_train)    \n",
    "    # vectorization\n",
    "    # Convert texts to vectors by TFIDF\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "    X_train_vec  = vectorizer.fit_transform(X_train_normalized)\n",
    "\n",
    "      \n",
    "    return X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72dabd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.53 s (started: 2022-04-17 11:21:13 +08:00)\n"
     ]
    }
   ],
   "source": [
    "trainDF = dc.get_train_with_noisy_df(4000, 1000, train_distribution)\n",
    "trainDF.reset_index(drop=True, inplace=True)\n",
    "X_train_vec = text_preprocessing_fordata(trainDF['message'].values)\n",
    "y_train = trainDF['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4839eaf8-d2db-4944-a289-8ca422c6e0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.56 ms (started: 2022-04-17 11:21:14 +08:00)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "# this is sparse matrix\n",
    "\n",
    "pick_train0 = X_train_vec[y_train==0]\n",
    "pick_train1 = X_train_vec[y_train==1]\n",
    "pick_train2 = X_train_vec[y_train==2]\n",
    "pick_train3 = X_train_vec[y_train==3]\n",
    "#pick_trainX_dense = pick_trainX.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fbb986-42b4-400c-9ad2-9f9c718fd6b6",
   "metadata": {},
   "source": [
    "pick_trainX is a sparse matrix, so the dimension is 84975 which is too high, so here I use svd to decrease the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2f0ed56e-2d4b-4c41-bf35-99a916a0e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2704.477346631238561 > -2737.984549939988028). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2663.312864263805750 > -2663.408213392162452). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2727.732394228222802 > -2754.762134667524151). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2719.142159164443001 > -2780.688103762576247). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2650.538388607822526 > -2651.050938780140768). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2709.511817799264008 > -2728.855184186973020). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2717.732893836014682 > -2777.800783737219717). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2651.839562740431575 > -2652.222385997852143). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2628.621544840727438 > -2632.062168475146791). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2701.339870792590773 > -2703.871627220879873). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2656.274532892057323 > -2661.864058160365857). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2755.302090182494794 > -2759.513324417675904). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2599.548172495436120 > -2599.720342459921540). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2710.265032395882372 > -2768.064314041669149). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2635.942112209255356 > -2638.862866913469588). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2729.443793119265138 > -2738.574102470704020). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2628.009929376136370 > -2631.045188049831722). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2680.795952122701692 > -2760.531402038463057). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2696.101577151357105 > -2717.300260349838936). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2621.484820405447863 > -2625.326210386456296). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2561.881962896268760 > -2571.441945598963684). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2712.630656184021063 > -2732.607287918525799). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2779.046385064484639 > -2786.992336361664002). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2588.844911274651167 > -2594.757792442967911). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2632.113419099403927 > -2632.657098599926940). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2703.990736236954490 > -2731.778135645698512). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2566.048523514479712 > -2570.669569525375664). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2659.863969833967985 > -2666.358587464183074). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2703.071837799218883 > -2706.926508127408397). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2633.608706751220780 > -2634.957660289344403). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2626.349053467888098 > -2631.417296833803448). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2711.990035754276960 > -2774.344996590266419). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2675.165127709548869 > -2676.874986180839187). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n",
      "/Users/guopei/miniforge3/envs/tf2/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-2604.333462546814189 > -2608.092740418104313). You may want to try with a higher value of support_fraction (current value: 0.799).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30.8 s (started: 2022-04-17 11:37:22 +08:00)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svd = SVD(400)\n",
    "svd.Process(pick_train0)\n",
    "# now I try to remove the noise\n",
    "# instantiate model\n",
    "model2 = EllipticEnvelope(contamination = 0.1, support_fraction=0.8) \n",
    "# fit model\n",
    "result = model2.fit_predict(svd.U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be4a9d37-7978-46da-93ba-a4fded923c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.06 ms (started: 2022-04-17 11:22:42 +08:00)\n"
     ]
    }
   ],
   "source": [
    "result[result == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e47ce37-2a94-452f-8dc1-87732a3d6bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(588,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.89 ms (started: 2022-04-17 11:22:45 +08:00)\n"
     ]
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "57d9f8a4-f2fe-4b96-9a9d-46426eca56a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.06 ms (started: 2022-04-17 11:39:28 +08:00)\n"
     ]
    }
   ],
   "source": [
    "trainDF2 = trainDF[y_train == 0]\n",
    "resultDF = pd.DataFrame(result, columns=['markasnoise'], index = trainDF2.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f161f248-0630-466b-8c45-4d82b387fbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(588, 5)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.4 ms (started: 2022-04-17 11:22:51 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# the noise is marked 1, otherwise \n",
    "resultDF[resultDF['markasnoise'] == 1] = 0\n",
    "resultDF[resultDF['markasnoise']==-1] = 1\n",
    "trainDF3 = trainDF2.join(resultDF)\n",
    "trainDF3[trainDF3['noise'] == trainDF3['markasnoise']].shape\n",
    "trainDF3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "60b5f82f-dd79-4498-9daa-abe49ab6ca5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.21 ms (started: 2022-04-17 11:43:42 +08:00)\n"
     ]
    }
   ],
   "source": [
    "def DetectNoise( trainDF, trainVec, y_train, sentimentType, dimension):\n",
    "    # I only pick the data with the specific type\n",
    "    pick_trainX = X_train_vec[y_train==sentimentType]    \n",
    "    svd = SVD(dimension)\n",
    "    svd.Process(pick_trainX)\n",
    "    # now I try to remove the noise\n",
    "    # instantiate model\n",
    "    model = EllipticEnvelope(contamination = 0.1, support_fraction=0.8) \n",
    "    # fit model\n",
    "    result = model.fit_predict(svd.U)\n",
    "    pick_trainDF = trainDF[y_train == sentimentType]\n",
    "    # create result data frame\n",
    "    resultDF = pd.DataFrame(result, columns=['markasnoise'], index = pick_trainDF.index)\n",
    "    # change the flag 0: not noise 1: noise\n",
    "    resultDF[resultDF['markasnoise'] == 1] = 0\n",
    "    resultDF[resultDF['markasnoise']==-1] = 1\n",
    "    # append markasnoise column to the train data frame\n",
    "    newTrainDF = pick_trainDF.join(resultDF)\n",
    "    return newTrainDF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "304d116b-b31c-43ab-8ffc-ecb82f2ae73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.85 ms (started: 2022-04-17 12:04:58 +08:00)\n"
     ]
    }
   ],
   "source": [
    "def CleanDFNoise(trainDF, X_train_vec, y_train, dimension):\n",
    "    trainDFs = []\n",
    "    \n",
    "    for sentiment in [0,1,2,3]:\n",
    "        # detect noise\n",
    "        trainDFForSentiment = DetectNoise(trainDF, X_train_vec, y_train, sentiment, dimension)\n",
    "        trainDFs.append(trainDFForSentiment)\n",
    "    # merge the dataframes\n",
    "    mergedDF = pd.concat(trainDFs)\n",
    "    # finally I drop the rows marked as noise\n",
    "    cleanDF = mergedDF[mergedDF['markasnoise'] == 0]\n",
    "    new_trainVec = X_train_vec[mergedDF['markasnoise'] == 0]\n",
    "    new_ytrain = y_train[mergedDF['markasnoise'] == 0]\n",
    "    return (cleanDF, new_trainVec, new_ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33822a-107e-42f9-a87e-71fda52fba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 400\n",
    "cleanDF, newTrainVec, newYTrain = CleanDFNoise(trainDF, X_train_vec, y_train, dimension )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab78a05-28af-417b-a02a-2c92ad460b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
